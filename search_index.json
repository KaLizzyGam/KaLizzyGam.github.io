[["index.html", "AMAT- Introducción a Ciencia de Datos y Machine Learning Capítulo 1 BIENVENIDA 1.1 Objetivo 1.2 ¿Quienes somos?", " AMAT- Introducción a Ciencia de Datos y Machine Learning Karina Lizette Gamboa Puente Oscar Arturo Bringas López Capítulo 1 BIENVENIDA 1.1 Objetivo La primera parte del curso tiene como finalidad que el alumno tenga un entendimiento general de conceptos, técnicas, algoritmos y del proceso de desarrollo de proyectos de Ciencia de Datos. Entenderá la diferencia entre Big Data, Machine Learning, Business Intelligence y Ciencia de Datos. Todo lo anterior será cumplido mientras el alumno aprende las paqueterías y funciones más novedosas que se usan en R para Ciencia de Datos y las tecnologías que dan soporte a este software. Se asume que el alumno tiene conocimientos generales de estadística, bases matemáticas y de programación básica en R. 1.2 ¿Quienes somos? ACT. ARTURO BRINGAS Actuario, egresado de la Facultad de Ciencias y Maestría en Ciencia de Datos, ITAM. Experiencia en el Departamento de Investigación Aplicada y Opinión de la UNAM diseñando metodologías estadísticas para estudios de percepción social. Consultor para empresas y organizaciones como GNP, CBX, El Universal, INEGI, UNAM e IZZI. Actualmente se desempeña en diferentes proyectos contribuyendo a empresas en temas de ciencia de datos, estadística, programación, visualización de datos y análisis geoespacial. ACT. LIZETTE GAMBOA Actuaria, egresada de la Facultad de Ciencias, UNAM, Maestría en Ciencia de Datos, ITAM. Experiencia en áreas de analítica predictiva, e inteligencia del negocio, Lead Data Scientist en consultoría en diferentes proyectos con empresas de tecnología, retail y del sector asegurador y financiero. Experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Consultoras actuales: CLOSTER y RakenDataGroup. Experiencia en GNP, Activer Banco y Casa de Bolsa, PlayCity Casinos, entre otros. "],["intro.html", "Capítulo 2 INTRODUCCIÓN 2.1 Lo que NO es Ciencia de Datos 2.2 Objetivo de la Ciencia se Datos 2.3 ¿Qué se requiere para hacer Ciencia de Datos? 2.4 Tipos de problemas que se pueden resolver con Ciencia de Datos 2.5 Tipos de aprendizaje 2.6 Ciclo de un proyecto de Ciencia de Datos", " Capítulo 2 INTRODUCCIÓN 2.1 Lo que NO es Ciencia de Datos 2.1.1 Definiendo conceptos: Estadistica Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. Estadística descriptiva: Describe de manera cuantificada información de los datos: distribuciones de los datos, EDA, outliers, etc. &lt;- lo veremos más adelante. Estadística inferencial: A partir de los datos de una población muestral deducir/concluir para una población. *Principales tipos de problemas: Todo lo que tenga datos -tener muchos datos, puede llegar a ser un problema-. La estadística supone -casi siempre- que el sistema es estático y generaliza la solución bajo las mismas condiciones. Si la cantidad de datos es de gran escala, es muy probable que se tengan que hacer muestras para trabajar con los datos. Machine Learning: El machine learning aprendizaje automático es una rama de la inteligencia artificial que permite que las máquinas aprendan sin ser expresamente programadas para ello. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPIs y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. (BI esta enfocado en analizar la historia pasada) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imagenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es comun que se confunda los conceptos de Big Data y Big Compute, como habiamos mencionado Big Data se refiere a el procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 2.2 Objetivo de la Ciencia se Datos Ciencia de datos: Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway 2.3 ¿Qué se requiere para hacer Ciencia de Datos? Background / Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría análitica, programación, conocimientos computacionales etc Datos Relevancia y suficiencia Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Etiquetas Se necesita la intervención humana para etiquetar, clasificar e introducir los datos en el algoritmo. Software Existen distintos lenguajes de programación para realizar ciencia de datos: 2.4 Tipos de problemas que se pueden resolver con Ciencia de Datos Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: 2.5 Tipos de aprendizaje La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta correcta y el objetivo de los algoritmos es predecir esta etiqueta. 2.5.1 Aprendizaje supervisado Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue etiquetada por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas bien. 2.5.1.1 Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Los algoritmos de clasificación se usan cuando el resultado deseado es una etiqueta discreta, es decir, clasifican un elemento dentro de diversas clases. En un problema de regresión, la variable target o variable a predecir es un valor numérico. 2.5.2 Aprendizaje no supervisado Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características 2.6 Ciclo de un proyecto de Ciencia de Datos Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes experimentos. Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. "],["transformacion-y-manipulacion-de-estructura-de-datos.html", "Capítulo 3 TRANSFORMACION Y MANIPULACION DE ESTRUCTURA DE DATOS 3.1 Importación de datos 3.2 Lectura de datos 3.3 Consultas de datos con tidyverse 3.4 Referencias:", " Capítulo 3 TRANSFORMACION Y MANIPULACION DE ESTRUCTURA DE DATOS Como se ha mencionado anteriormente, existe un ciclo dentro de un proyecto de Ciencia de Datos que aborda desde la identificación del problema a resolver, hasta la definición de las acciones que serán detonadas a partir del resultado de un modelo validado. Para efectos de este curso, tendremos el supuesto de que éstas definiciones y alcances ya están previamente decididos o en otro caso muy probable; no dependen de nosotros. Así, el curso estará enfocado al flujo de trabajo técnico que conlleva el análisis de los datos en la que se buscará la solución del problema. Particularmente, R tiene bibliotecas especializadas que pueden ayudarnos en cada parte del camino. 3.1 Importación de datos Existen diferentes archivos de los cuales puede recolectarse información, los más comunes son La paquetería base de R, contiene las funciones que eran la forma más común de introducir datos en R: read.csv(file, header = TRUE, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) read.table(file, header = FALSE, sep = &quot;&quot;, quote = &quot;\\&quot;&#39;&quot;, dec = &quot;.&quot;, numerals = c(&quot;allow.loss&quot;, &quot;warn.loss&quot;, &quot;no.loss&quot;), row.names, col.names, as.is = !stringsAsFactors, na.strings = &quot;NA&quot;, colClasses = NA, nrows = -1, skip = 0, check.names = TRUE, fill = !blank.lines.skip, strip.white = FALSE, blank.lines.skip = TRUE, comment.char = &quot;#&quot;, allowEscapes = FALSE, flush = FALSE, stringsAsFactors = default.stringsAsFactors(), fileEncoding = &quot;&quot;, encoding = &quot;unknown&quot;, text, skipNul = FALSE) Sin embargo, las funciones anteriores importan los marcos de datos a R de la forma antigua, por otro lado, la paquetería readr fue desarrollado recientemente para lidiar con la lectura de archivos grandes rápidamente. El paquete proporciona reemplazos para funciones como read.table(), read.csv() entre otras. Esta paquetería proporciona funciones que suelen ser mucho más rápidas que las funciones base que proporciona R. Ventajas de readr: Por lo general, son mucho más rápidos (~ 10x) que sus funciones equivalentes. Producen tibbles: No convierten vectores de caracteres en factores. No usan nombres de filas ni modifican los nombres de columnas. Reproducibilidad 3.2 Lectura de datos 3.2.1 Archivos csv A la hora de importar conjuntos de datos en R, uno de los formatos más habituales en los que hallamos información es en archivos separados por comas (comma separated values), cuya extensión suele ser .csv. En ellos encontramos múltiples líneas que recogen la tabla de interés, y en las cuales los valores aparecen, de manera consecutiva, separados por el carácter ,. Para importar este tipo de archivos en nuestra sesión de R, se utiliza la función read_csv(). Para acceder a su documentación utilizamos el comando ?read_csv. El único argumento que debemos de pasar a esta función de manera obligatoria, es file, el nombre o la ruta completa del archivo que pretendemos importar. library(readr) read_csv( file, col_names = TRUE, col_types = NULL, locale = default_locale(), na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;) Algunas de las ventajas que utilizar read_csv() ofrece son: No convierte, automáticamente, las columnas con cadenas de caracteres a factores, como sí hacen por defecto las otras funciones base de R. Reconoce ocho clases diferentes de datos (enteros, lógicos, etc.), dejando el resto como cadenas de caracteres. Veamos un ejemplo: La base de datos llamada AmesHousing contiene un conjunto de datos con información de la Oficina del Tasador de Ames utilizada para calcular los valores tasados para las propiedades residenciales individuales vendidas en Ames, Iowa, de 2006 a 2010. FUENTES: Ames, Oficina del Tasador de Iowa. Pueden descargar los datos para la clase aquí base&lt;- read.csv(&quot;data/ames.csv&quot;) base tidy&lt;- read_csv(&quot;data/ames.csv&quot;) tidy ¿Y si el archivo que necesitamos leer esta en excel? 3.2.2 Archivos xls y xlsx La paquetería readxl facilita la obtención de datos tabulares de archivos de Excel. Admite tanto el formato .xls heredado como el formato .xlsx moderno basado en XML. Esta paqueteía pone a disposición las siguientes funciones: read_xlsx() lee un archivo con extensión xlsx. read_xlsx(path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot;) read_xls() lee un archivo con extensión xls. read_xls(path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot;) read_excel() determina si el archivo es de tipo xls o xlsx para después llamar a una de las fuciones mencionadas anteriormente. read_excel(path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot;) EJERCICIO: Leer archivo excel de la carpeta del curso 3.2.3 Archivos json Se utiliza la función fromJSON de la paquetería rjson library(rjson) base_json &lt;- fromJSON(&quot;data/ames.json&quot;) base_json 3.2.4 Bases de Datos En muchos de los casos la información estará dentro de un Sistema Manejador de Bases de Datos, existen bibliotecas que nos permiten establecer las conexiones con ellas, algunos ejemplos son: ODBC DBI Un ejemplo con un SMBD como SQL Server: 3.2.4.1 Microsoft SQL Server Referencias Configuración de conexión: Se necesitan seis configuraciones para realizar una conexión: Controlador : consulte la sección Controladores para obtener más información Servidor : una ruta de red al servidor de la base de datos. Base de datos : el nombre de la base de datos. UID : el ID de red del usuario o la cuenta local del servidor PWD : la contraseña de la cuenta Puerto : debe establecerse en 1433 Para establecer la conexión con la base de datos: library(DBI) con &lt;- DBI::dbConnect(odbc::odbc(), Driver = &quot;[your driver&#39;s name]&quot;, Server = &quot;[your server&#39;s path]&quot;, Database = &quot;[your database&#39;s name]&quot;, UID = rstudioapi::askForPassword(&quot;Database user&quot;), PWD = rstudioapi::askForPassword(&quot;Database password&quot;), Port = 1433) Información sobre la base de datos: El odbcpaquete le brinda herramientas para explorar objetos y columnas en la base de datos. # Top level objects odbcListObjects(con) # Tables in a schema odbcListObjects(con, catalog=&quot;mydb&quot;, schema=&quot;dbo&quot;) # Columns in a table odbcListColumns(con, catalog=&quot;mydb&quot;, schema=&quot;dbo&quot;, table=&quot;cars&quot;) # Database structure odbcListObjectTypes(con) Consultas con SQL: Para consultas interactivas, utilice dbGetQuery()para enviar una consulta y obtener los resultados. Para obtener los resultados por separado, utilice dbSendQuery()y dbFetch(). El n=argumento en dbFetch()se puede utilizar para obtener resultados parciales. # Return the results for an arbitrary query dbGetQuery(con, &quot;SELECT speed, dist FROM cars&quot;) # Fetch the first 100 records query &lt;- dbSendQuery(con, &quot;SELECT speed, dist FROM cars&quot;) dbFetch(query, n = 10) dbClearResult(query) Puedes usar los ejemplos anteriores para probar con diferentes consultas y bases de datos. Tidyverse a SQL Tengamos un ejemplo de manera local: remotes::install_version(&quot;RSQLite&quot;, version = &quot;2.2.5&quot;) library(dplyr) library(dbplyr) library(RSQLite) con &lt;- src_memdb() copy_to(con, storms, overwrite = T) tbl_storms &lt;- tbl(con, &quot;storms&quot;) query &lt;- tbl_storms %&gt;% select(name, year, month, day, hour, status, wind, pressure) %&gt;% group_by(status) %&gt;% summarise( mean_wind = mean(wind, na.rm = T), sd_wind = sd(wind, na.rm = T), mean_pressure = mean(pressure, na.rm = T), se_pressure = sd(pressure, na.rm = T) ) %&gt;% arrange(mean_wind) %&gt;% right_join(tbl_storms, by = &quot;status&quot;) %&gt;% # relocate(name, .after = status) %&gt;% # relocate(wind, .after = mean_pressure) %&gt;% # relocate(pressure, .after = wind) %&gt;% select(-lat, -long, -hu_diameter, -ts_diameter) query %&gt;% show_query() ## &lt;SQL&gt; ## SELECT `status`, `mean_wind`, `sd_wind`, `mean_pressure`, `se_pressure`, `name`, `year`, `month`, `day`, `hour`, `category`, `wind`, `pressure` ## FROM (SELECT `RHS`.`status` AS `status`, `mean_wind`, `sd_wind`, `mean_pressure`, `se_pressure`, `name`, `year`, `month`, `day`, `hour`, `lat`, `long`, `category`, `wind`, `pressure`, `ts_diameter`, `hu_diameter` ## FROM `storms` AS `RHS` ## LEFT JOIN (SELECT `status`, AVG(`wind`) AS `mean_wind`, STDEV(`wind`) AS `sd_wind`, AVG(`pressure`) AS `mean_pressure`, STDEV(`pressure`) AS `se_pressure` ## FROM (SELECT `name`, `year`, `month`, `day`, `hour`, `status`, `wind`, `pressure` ## FROM `storms`) ## GROUP BY `status`) AS `LHS` ## ON (`LHS`.`status` = `RHS`.`status`) ## ) Para poder revisar la consulta, puedes ejecutar: query collect(query) 3.3 Consultas de datos con tidyverse Ahora que ya sabemos como cargar datos, aprenderemos como manipularlos con dplyr. El paquete dplyr proporciona un conjunto de funciones muy útiles para manipular data frames y así reducir el número de repeticiones, la probabilidad de cometer errores y el número de caracteres que hay que escribir. Como valor extra, puedes encontrar que la gramática de dplyr es más fácil de entender. Revisaremos algunas de sus funciones más usadas (verbos), así como el uso de pipes (%&gt;%) para combinarlas. select() filter() arrange() mutate() summarise() join() group_by() Primero tenemos que instalar y cargar la paquetería (parte de tidyverse): # install.packages(&quot;dplyr&quot;) library(dplyr) Usaremos el dataset iris que se encuentra en la paquetería datasets (el alumno puede hacer el ejercicio con la base AmesHousing) head(iris,10) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa 3.3.1 select() Observamos que nuestros datos tienen 150 observaciones y 5 variables, con select() podemos seleccionar las variables que le indiquemos. select_ejemplo&lt;-iris %&gt;% select(Sepal.Length,Petal.Length,Species) head(select_ejemplo,10) ## Sepal.Length Petal.Length Species ## 1 5.1 1.4 setosa ## 2 4.9 1.4 setosa ## 3 4.7 1.3 setosa ## 4 4.6 1.5 setosa ## 5 5.0 1.4 setosa ## 6 5.4 1.7 setosa ## 7 4.6 1.4 setosa ## 8 5.0 1.5 setosa ## 9 4.4 1.4 setosa ## 10 4.9 1.5 setosa El operador pipe (%&gt;%) se usa para conectar múltiples acciones, en este caso solo le indicamos que de la base iris seleccionara 3 varibles y que guardara este nuevo data frame en la variable select_ejemplo. Con select() y contains podemos seleccionar varibles con alguna cadena de texto. select_ejemplo&lt;-iris %&gt;% select(contains(&quot;Sepal&quot;)) head(select_ejemplo,5) ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 De igual manera, con select(), ends_with y start_with() podemos seleccionar que inicien o terminen con alguna cadena de texto. select_ejemplo&lt;-iris %&gt;% select(starts_with(&quot;Sepal&quot;),ends_with(&quot;Length&quot;)) head(select_ejemplo,5) ## Sepal.Length Sepal.Width Petal.Length ## 1 5.1 3.5 1.4 ## 2 4.9 3.0 1.4 ## 3 4.7 3.2 1.3 ## 4 4.6 3.1 1.5 ## 5 5.0 3.6 1.4 Funciones útiles para select(): contains(): Selecciona variables cuyo nombre contiene la cadena de texto. ends_with(): Selecciona variables cuyo nombre termina con la cadena de caracteres. everything(): Selecciona todas las columnas. matches(): Selecciona las variables cuyos nombres coinciden con una expresión regular. num_range(): Selecciona las variables por posición. one_of(): Selecciona variables cuyos nombres están en un grupo de nombres. start_with(): Selecciona variables cuyos nombres empiezan con la cadena de caracteres. 3.3.2 filter() La función filter nos permite filtrar filas según una condición, primero notemos que la variable Species tiene tres categorías. table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 Ahora usaremos la función filter para quedarnos solo con las observaciones de la especie virginica. ejemplo_filter&lt;- iris %&gt;% filter(Species==&quot;virginica&quot;) head(ejemplo_filter,5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.3 3.3 6.0 2.5 virginica ## 2 5.8 2.7 5.1 1.9 virginica ## 3 7.1 3.0 5.9 2.1 virginica ## 4 6.3 2.9 5.6 1.8 virginica ## 5 6.5 3.0 5.8 2.2 virginica También se puede usar para filtrar variables numéricas: ejemplo_filter&lt;- iris %&gt;% filter(Sepal.Length&gt;5 &amp; Sepal.Width&gt;=3.5) head(ejemplo_filter,5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 5.4 3.9 1.7 0.4 setosa ## 3 5.4 3.7 1.5 0.2 setosa ## 4 5.8 4.0 1.2 0.2 setosa ## 5 5.7 4.4 1.5 0.4 setosa Notemos que en el ejemplo anterior usamos &amp;, que nos ayuda a filtrar por dos condiciones. También podemos usar | para filtrar por alguna de las dos condiciones. ejemplo_filter&lt;- iris %&gt;% filter(Sepal.Length&gt;5 | Sepal.Width&gt;=3.5) head(ejemplo_filter,5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 5.0 3.6 1.4 0.2 setosa ## 3 5.4 3.9 1.7 0.4 setosa ## 4 5.4 3.7 1.5 0.2 setosa ## 5 5.8 4.0 1.2 0.2 setosa Las condiciones pueden ser expresiones lógicas construidas mediante los operadores relacionales y lógicos: &lt; : Menor que &gt; : Mayor que = : Igual que &lt;= : Menor o igual que &gt;= : Mayor o igual que != : Diferente que %in% : Pertenece al conjunto is.na : Es NA !is.na : No es NA 3.3.3 arrange() La función arrange() se utiliza para ordenar las filas de un data frame de acuerdo a una o varias variables. Por defecto arrange() ordena las filas por orden ascendente: ejemplo_arrange&lt;- iris %&gt;% arrange(Sepal.Length) head(ejemplo_arrange,10) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.3 3.0 1.1 0.1 setosa ## 2 4.4 2.9 1.4 0.2 setosa ## 3 4.4 3.0 1.3 0.2 setosa ## 4 4.4 3.2 1.3 0.2 setosa ## 5 4.5 2.3 1.3 0.3 setosa ## 6 4.6 3.1 1.5 0.2 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 4.6 3.6 1.0 0.2 setosa ## 9 4.6 3.2 1.4 0.2 setosa ## 10 4.7 3.2 1.3 0.2 setosa Si las queremos ordenar de forma ascendente lo haremos del siguiente modo: ejemplo_arrange&lt;- iris %&gt;% arrange(desc(Sepal.Length)) head(ejemplo_arrange,10) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 7.9 3.8 6.4 2.0 virginica ## 2 7.7 3.8 6.7 2.2 virginica ## 3 7.7 2.6 6.9 2.3 virginica ## 4 7.7 2.8 6.7 2.0 virginica ## 5 7.7 3.0 6.1 2.3 virginica ## 6 7.6 3.0 6.6 2.1 virginica ## 7 7.4 2.8 6.1 1.9 virginica ## 8 7.3 2.9 6.3 1.8 virginica ## 9 7.2 3.6 6.1 2.5 virginica ## 10 7.2 3.2 6.0 1.8 virginica 3.3.4 mutate() Con la función mutate() podemos computar tranformaciones de variables en un data frame. A menudo, tendremos la necesidad de crear nuevas variables que se calculan a partir de variables existentes,mutate() nos proporciona una interface clara para realizar este tipo de operaciones. Por ejemplo haremos la suma de las variables Sepal.Lenght y Sepal.Width: ejemplo_mutate&lt;- iris %&gt;% select(Sepal.Length,Sepal.Width) %&gt;% mutate(Suma=Sepal.Length+Sepal.Width) head(ejemplo_mutate,5) ## Sepal.Length Sepal.Width Suma ## 1 5.1 3.5 8.6 ## 2 4.9 3.0 7.9 ## 3 4.7 3.2 7.9 ## 4 4.6 3.1 7.7 ## 5 5.0 3.6 8.6 Notemos que en el ejemplo anterior usamos dos pipes (%&gt;%), como habiamos mencionado se pueden usar los necesarios para combinar funciones. 3.3.5 summarise() La función summarise() funciona de forma análoga a la función mutate, excepto que en lugar de añadir nuevas columnas crea un nuevo data frame. Podemos usar el ejemplo anterior y calcular la media de la varible creada Suma: ejemplo_summarise&lt;- iris %&gt;% select(Sepal.Length,Sepal.Width) %&gt;% mutate(Suma=Sepal.Length+Sepal.Width) %&gt;% summarise(Media_Suma=mean(Suma)) ejemplo_summarise ## Media_Suma ## 1 8.900667 Solo fue necesario agregar un pipe, especificar el nombre de la varible creada y la operación a realizar. A continuación se muestran funciones que trabajando conjuntamente con la función summarise() facilitarán nuestro trabajo diario. Las primeras pertenecen al paquete base y las otras son del paquete dplyr. Todas ellas toman como argumento un vector y devuelven un único resultado: min(), max() : Valores max y min. mean() : Media. median() : Mediana. sum() : Suma de los valores. var(), sd() : Varianza y desviación estandar. first() : Primer valor en un vector. last() : El último valor en un vector n() : El número de valores en un vector. n_distinc() : El número de valores distintos en un vector. nth() : Extrar el valor que ocupa la posición n en un vector. Mas adelante veremos como combinar esta función con la función group_by. 3.3.6 group_by() La función group_by() agrupa un conjunto de filas seleccionado en un conjunto de filas de resumen de acuerdo con los valores de una o más columnas o expresiones. Usaremos el ejemplo anterior, primero creamos nuestra nueva variable Suma, despues agrupamos por especie y al final sacamos la media de la variable Suma pero esta vez, por la función group_by, nos regresara una media por cada grupo creado, es decir, nos regresara el promedio de la suma por especie. ejemplo_groupby&lt;- iris %&gt;% mutate(Suma=Sepal.Length+Sepal.Width) %&gt;% group_by(Species) %&gt;% summarise(Media_Suma=mean(Suma)) ejemplo_groupby ## # A tibble: 3 x 2 ## Species Media_Suma ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 8.43 ## 2 versicolor 8.71 ## 3 virginica 9.56 3.3.7 join() Existen 4 funciones para combinar data frames: inner_join(): Regresa todas las observaciones de x que tiene valores que coinciden en y, regresa todas las columnas de x y y left_join(): Regresa todas las observaciones de x y las columnas de x y y. Observaciones en x que no esten en y tendran NAs en sus nuevas columnas. right_join(): Regresa todas las observaciones de y y las columnas de y y x. Observaciones en y que no esten en x tendran NAs en sus nuevas columnas. full_join(): Regresa todas las columnas de x y y Usaremos los datasets band_instruments y band_members para mostrar algunos ejemplos. band_instruments ## # A tibble: 3 x 2 ## name plays ## &lt;chr&gt; &lt;chr&gt; ## 1 John guitar ## 2 Paul bass ## 3 Keith guitar band_members ## # A tibble: 3 x 2 ## name band ## &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones ## 2 John Beatles ## 3 Paul Beatles En este caso la función inner_join() nos devuelve todas las varibles con las observaciones que tienen texto igual en la varible name de los data frames band_members y band_instruments. inner_join(band_members,band_instruments, by = &quot;name&quot;) ## # A tibble: 2 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Beatles guitar ## 2 Paul Beatles bass Ahora con la función left_join nos regresa todas las observaciones de band_members y todas las varibles de ambos data frames, con NAs en las observaciones de band_members que no tengan un valor en la varible plays de band_instruments. left_join(band_members,band_instruments, by = &quot;name&quot;) ## # A tibble: 3 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones &lt;NA&gt; ## 2 John Beatles guitar ## 3 Paul Beatles bass La función right_join funciona igual que left_join solo qur ahora toma las observaciones de band_instrument. right_join(band_members,band_instruments, by = &quot;name&quot;) ## # A tibble: 3 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Beatles guitar ## 2 Paul Beatles bass ## 3 Keith &lt;NA&gt; guitar La función full_join junta todas las obervaciones de ambos data frames poninedo NAs donde no se tenga un valor. full_join(band_members,band_instruments, by = &quot;name&quot;) ## # A tibble: 4 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones &lt;NA&gt; ## 2 John Beatles guitar ## 3 Paul Beatles bass ## 4 Keith &lt;NA&gt; guitar tidyr EL objetivo de tidyr es crear tidy data, un formato que hace que el manejo de los datos sea más semcillo, en esta ocasión revisaremos dos funciones, pivot_longer() y pivot_wider() 3.3.8 pivot_longer() La función pivot_longer sirve para incrementar el número de renglones y disminuir el número de columnas, uasaremos el dataset table4a de la paquetería tidyr. table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 3.3.8.1 Ejemplo pivot_longer() Un problema común es cuando en un dataset los nombres de las columnas no representan nombres de variables, sino que representan los valores de una variable, en nuestro caso los nombres de las columnas 1999 y 2000 representan los valores de la variable año, los valores en las columnas 1999 y 2000 representan valores de la variable casos y cada fila representa dos observaciones en lugar de una. Los parámetros que usaremos para pivotear son: El conjunto de columnas cuyos nombres son valores y no variables. En este ejemplo son las columnas 1999 y 2000. El nombre de la variable cuyos valores forman los nombres de las columnas. Llamaremos a esto key (clave) y en este caso corresponde a anio. El nombre de la variable cuyos valores están repartidos por las celdas. Llamaremos a esto value (valor) y en este caso corresponde al número de casos. 3.3.8.2 Ejemplo pivot_longer() table4a %&gt;% pivot_longer( cols = c(`1999`, `2000`), names_to = &quot;año&quot;, values_to = &quot;casos&quot; ) ## # A tibble: 6 x 3 ## country año casos ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 3.3.9 pivot_wider() La función pivot_wider sirve para incrementar el número de columnas y disminuir el número de renglones, uasaremos el dataset table4a de la paquetería tidyr. la función pivot_wider() es lo opuesto de pivot_longer(). Se usa cuando una observación aparece en múltiples filas. Por ejemplo, considera la tabla2: una observación es un país en un año, pero cada observación aparece en dos filas. table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 3.3.9.1 Ejemplo pivot_wider() Para ordenar esto, primero analizaremos la representación, esta vez, solo necesitamos dos parámetros: La columna desde la que obtener los nombres de las variables. En este caso corresponde a tipo. La columna desde la que obtener los valores. En este caso corresponde a cuenta. table2 %&gt;% pivot_wider( names_from = type, values_from = count ) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 3.4 Referencias: Diferentes tipos de datos JSON jsonlite XML Leer XML con url dplyr R for Data Science "],["analisis-exploratorio-y-visualización.html", "Capítulo 4 ANALISIS EXPLORATORIO Y VISUALIZACIÓN 4.1 EDA: Análisis Exploratorio de Datos 4.2 GEDA: Análisis Exploratorio de Datos Gráficos 4.3 Uso, decisión e implementación de técnicas gráficas. 4.4 Ggplot2 4.5 EDA y GEDA con R 4.6 Referencias:", " Capítulo 4 ANALISIS EXPLORATORIO Y VISUALIZACIÓN El análisis exploratorio de datos se refiere al proceso crítico de realizar investigaciones iniciales sobre los datos para descubrir patrones, detectar anomalías, probar hipótesis y verificar suposiciones con la ayuda de estadísticas resumidas y representaciones gráficas. Towards 4.1 EDA: Análisis Exploratorio de Datos Un analisis explorario de datos tiene principalmente 5 objetivos: Maximizar el conocimiento de un conjunto de datos Descubrir la estructura subyacente de los datos Extraer variables importantes Detectar valores atípicos y anomalías Probar los supuestos subyacentes EDA no es idéntico a los gráficos estadísticos aunque los dos términos se utilizan casi indistintamente. Los gráficos estadísticos son una colección de técnicas, todas basadas en gráficos y todas centradas en un aspecto de caracterización de datos. EDA abarca un lugar más grande. EDA es una filosofía sobre cómo diseccionar un conjunto de datos; lo que buscamos; cómo nos vemos; y cómo interpretamos. Los científicos de datos pueden utilizar el análisis exploratorio para garantizar que los resultados que producen sean válidos y aplicables a los resultados y objetivos comerciales deseados. EDA se utiliza principalmente para ver qué datos pueden revelar más allá del modelado formal o la tarea de prueba de hipótesis y proporciona una mejor comprensión de las variables del conjunto de datos y las relaciones entre ellas. También puede ayudar a determinar si las técnicas estadísticas que está considerando para el análisis de datos son apropiadas. Dependiendo del tipo de variable queremos obtener la siguiente información: Variables numéricas: Tipo de dato: float, integer Número de observaciones Mean Desviación estándar Cuartiles: 25%, 50%, 75% Valor máximo Valor mínimo Número de observaciones únicos Top 5 observaciones repetidas Número de observaciones con valores faltantes ¿Hay redondeos? Variables categóricas Número de categorías Valor de las categorías Moda Valores faltantes Número de observaciones con valores faltantes Proporción de observaciones por categoría Top 1, top 2, top 3 (moda 1, moda 2, moda 3) Faltas de ortografía ? Fechas Fecha inicio Fecha fin Huecos en las fechas: sólo tenemos datos entre semana, etc. Formatos de fecha (YYYY-MM-DD) Tipo de dato: date, time, timestamp Número de faltantes (NA) Número de observaciones Texto Longitud promedio de cada observación Identificar el lenguaje, si es posible Longitud mínima de cada observación Longitud máxima de cada observación Cuartiles de longitud: 25%, 50%, 75% Coordenadas geoespaciales Primero se pone la latitud y luego la longitud Primer decimal: 111 kms Segundo decimal: 11.1 kms Tercer decimal: 1.1 kms Cuarto decimal: 11 mts Quinto decimal: 1.1 mt Sexto decimal: 0.11 mts Valores que están cercanos al 100 representan la longitud El símbolo en cada coordenada representa si estamos al norte (positivo) o sur (negativo) -en la latitud-, al este (positivo) o al - oeste (negativo) -en la longitud-. 4.2 GEDA: Análisis Exploratorio de Datos Gráficos Como complemento al EDA podemos realizar un GEDA, que es un análisis exploratorio de los datos apoyándonos de visualizaciones, la visualización de datos no trata de hacer gráficas bonitas o divertidas, ni de simplificar lo complejo. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. 4.2.1 Lo que no se debe hacer Fuentes: WTF Visualizations Flowingdata {r echo=FALSE,fig.align=center, out.width = 700pt} knitr::include_graphics(img/viz/bad_viz3.png) 4.2.2 Principios de visualización El objetivo de una visualización es sintentizar información relevante al análisis presentada de manera sencilla y sin ambigüedad. Lo usamos de apoyo para explicar a una audiencia más amplia que puede no ser tan técnica. Una gráfica debe reportar el resultado de un análisis detallado, nunca lo reemplaza. No hacer gráficas porque se vean cool Antes de hacer una gráfica, debe pensarse en lo que se quiere expresar o representar Existen reglas o mejores gráficas para representar cierto tipo de información de acuerdo a los tipos de datos que se tienen o al objetivo se quiere lograr con la visualización. From Data to Viz No utilizar pie charts 4.2.3 Principios generales del diseño analítico: Muestra comparaciones, contrastes, diferencias. Muestra causalidad, mecanismo, explicación. Muestra datos multivariados, es decir, más de una o dos variables. Integra palabras, números, imágenes y diagramas. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar: 4.2.4 Técnicas de visualización: Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, suavizamiento y bandas de confianza. 4.2.5 Indicadores de calidad gráfica: Aplicables a cualquier gráfica en particular, son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica. Integridad Gráfica: El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk: Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos: Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: Densidad de datos: Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. 4.3 Uso, decisión e implementación de técnicas gráficas. 4.3.1 Gráficos univariados: Histograma: El histograma es la forma más popular de mostrar la forma de un conjunto de datos. Se divide la escala de la variable en intervalos, y se realiza un conteo de los casos que caen en cada uno de los intervalos. Los histogramas pueden mostrar distintos aspectos de los datos dependiendo del tamaño y posición de los intervalos. Diagramas de caja y brazos: Es un método estandarizado para representar gráficamente una serie de datos numéricos a través de sus cuartiles. El diagrama de caja muestra a simple vista la mediana y los cuartiles de los datos, pudiendo también representar los valores atípicos de estos. Gráficas de barras: Una gráfica de este tipo nos muestra la frecuencia con la que se han observado los datos de una variable discreta, con una barra para cada categoría de esta variable. Gráficos Circulares (Pie Charts): Un gráfico circular o gráfica circular, también llamado gráfico de pastel, es un recurso estadístico que se utiliza para representar porcentajes y proporciones. 4.3.2 Gráficos multivariados Gráfico de dispersión: Los gráficos de dispersión se usan para trazar puntos de datos en un eje vertical y uno horizontal, mediante lo que se trata de mostrar cuánto afecta una variable a otra. Si no existe una variable dependiente, cualquier variable se puede representar en cada eje y el diagrama de dispersión mostrará el grado de correlación (no causalidad) entre las dos variables. Gráficas de líneas: Uno de los tipos de gráfica más utilizados es la de líneas, especialmente cuando se quieren comparar visualmente varias variables a lo largo del tiempo o algún otro parámetro. 4.4 Ggplot2 Comparando con los gráficos base de R, ggplot2: Tiene una gramática más compleja para gráficos simples Tiene una gramática menos compleja para gráficos complejos o muy customizados. Los datos siempre deben ser un data.frame. Usa un sistema diferente para añadir elementos al gráfico. Histograma con los gráficos base: data(iris) hist(iris$Sepal.Length) Histograma con ggplot2: library(ggplot2) ggplot(iris, aes(x = Sepal.Length)) + geom_histogram(color = &#39;white&#39;, bins=8) Ahora vamos a ver un gráfico con colores y varias series de datos. Con los gráficos base: plot(Sepal.Length ~ Sepal.Width, col = factor(Species), data = iris) Con ggplot2: ggplot(iris, aes(x=Sepal.Width , y= Sepal.Length, color=Species))+ geom_point() 4.4.1 Objetos aesteticos En ggplot2, aestético significa algo que puedes ver. Algunos ejemplos son: Posición (por ejemplo, los ejes x e y) Color (color externo) Fill (color de relleno) Shape (forma de puntos) Linetype (tipo de linea) Size (tamaño) Alpha (para la transparencia: los valores más altos tendrían formas opacas y los más bajos, casi transparentes). Hay que advertir que no todas las estéticas tienen la misma potencia en un gráfico. El ojo humano percibe fácilmente longitudes distintas. Pero tiene problemas para comparar áreas (que es lo que regula la estética size) o intensidades de color. Se recomienda usar las estéticas más potentes para representar las variables más importantes. Cada tipo de objeto geométrico (geom) solo acepta un subconjunto de todos los aestéticos. Puedes consultar la pagina de ayuda de geom() para ver que aestéticos acepta. El mapeo aestético se hace con la función aes(). 4.4.2 Objetos geométricos o capas Los objetos geométricos son las formas que puede tomar un gráfico. Algunos ejemplos son: Barras (geom_bar(), para las variables univariados discretos o nominales) Histogramas (geom_hist() para aquellas variables univariadas continuas) Puntos (geom_point() para scatter plots, gráficos de puntos, etc) Lineas (geom_line() para series temporales, lineas de tendencia, etc) Cajas (geom_boxplot() para gráficos de cajas) Un gráfico debe tener al menos un geom, pero no hay limite. Puedes añadir más geom usando el signo +. Si queremos conocer la lista de objetos geométricos podemos ejercutar el siguiente código: help.search(&quot;geom_&quot;, package = &quot;ggplot2&quot;) Una vez añadida una capa al gráfico a este pueden agregarse nuevas capaas ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point() ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point()+ geom_smooth() 4.4.3 Facetas Muchos de los gráficos que pueden generarse con los elementos anteriores pueden reproducirse usando los gráficos tradicionales de R, pero no los que usan facetas, que pueden permitirnos explorar las variables de diferente forma, por ejemplo: ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) + geom_point() + geom_smooth() + facet_grid(~ Species) crea tres gráficos dispuestos horizontalmente que comparan la relación entre la anchura y la longitud del pétalo de las tres especies de iris. Una característica de estos gráficos, que es crítica para poder hacer comparaciones adecuadas, es que comparten ejes. 4.4.4 Más sobre estéticas Las estéticas se pueden etiquetar con la función labs. Además, se le puede añadir un título al gráfico usando la función ggtitle. Por ejemplo, en el gráfico anterior se pueden reetiquetar los ejes y la leyenda haciendo 4.5 EDA y GEDA con R Para los ejercicios en clase utilizaremos el set de datos: Diamonds: library(dplyr) library(ggplot2) library(reshape2) data(&quot;diamonds&quot;) Descripción Un conjunto de datos que contiene los precios y otros atributos de casi 54.000 diamantes. Las variables son las siguientes: price: precio en dólares estadounidenses ( $ 326 -  $ 18,823) carat: peso del diamante (0.25.01) cut: calidad del corte (Regular, Bueno, Muy Bueno, Premium, Ideal) color: color del diamante, de D (mejor) a J (peor) clarity: una medida de la claridad del diamante (I1 (peor), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (mejor)) x: longitud en mm (0-10,74) y: ancho en mm (058,9) width in mm (058.9) -z: profundidad en mm (031,8) depth porcentaje de profundidad total = z / media (x, y) = 2 * z / (x + y) (4379) table: ancho de la parte superior del diamante en relación con el punto más ancho (4395) Ejemplo práctico: diamonds %>% ggplot() + aes(x = cut_number(carat, 5), y = price) + geom_boxplot() + aes(color = cut) + labs(title = 'Distribución de precio por categoría de corte') + labs(caption = 'Data source:Diamont set') + labs(x = 'Peso del diamante') + labs(y = 'Precio') + guides(color = guide_legend(title = 'Calidad del corte')) + ylim(0, 20000) + scale_y_continuous( labels = scales::dollar_format(), breaks = seq(0, 20000,2500 ), limits = c(0, 20000) ) 4.5.1 Un vistazo rápido a los datos library(DataExplorer) plot_intro(diamonds) plot_missing(diamonds) 4.5.2 Análisis univariado 4.5.2.1 Variables numéricas Los histogramas son gráficas de barras que se obtienen a partir de tablas de frecuencias, donde cada barra se escala según la frecuencia relativa entre el ancho del intervalo de clase correspondiente. Un histograma muestra la acumulación ó tendencia, la variabilidad o dispersión y la forma de la distribución. El Diagrama de Caja y bigoteses un tipo de gráfico que muestra un resumen de una gran cantidad de datos en cinco medidas descriptivas, además de intuir su morfología y simetría. Este tipo de gráficos nos permite identificar valores atípicos y comparar distribuciones. Además de conocer de una forma cómoda y rápida como el 50% de los valores centrales se distribuyen. Se puede detectar rápidamente los siguientes valores: Primer cuartil: el 25% de los valores son menores o igual a este valor (punto 2 en el gráfico anterior). Mediana o Segundo Cuartil: Divide en dos partes iguales la distribución. De forma que el 50% de los valores son menores o igual a este valor (punto 3 en el gráfico siguiente). Tercer cuartil: el 75% de los valores son menores o igual a este valor (punto 4 en el gráfico siguiente). Rango Intercuartílico (RIC): Diferencia entre el valor del tercer cuartil y el primer cuartil. Tip: El segmento que divide la caja en dos partes es la mediana (punto 3 del gráfico), que facilitará la comprensión de si la distribución es simétrica o asimétrica, si la mediana se sitúa en el centro de la caja entonces la distribución es simétrica y tanto la media, mediana y moda coinciden. Precio diamonds %&gt;% ggplot( aes( x = price)) + geom_histogram( aes(y = ..density..), color= &quot;Blue&quot;, fill= &quot;White&quot;, bins = 30 ) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1)+ scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = price)) + geom_boxplot(binwidth = 1000, color= &quot;Blue&quot;, fill= &quot;White&quot;) + scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de precio&quot;) Peso del diamante diamonds %&gt;% ggplot( aes( x = carat)) + geom_histogram(binwidth = .03, color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() diamonds %&gt;% ggplot( aes( x = carat)) + geom_boxplot(binwidth = .3, color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() 4.5.2.2 Variables nominales/categóricas Cálidad de corte diamonds %&gt;% ggplot( aes( x = cut)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;cyan&quot;, alpha= 0.7) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de cálidad de corte&quot;) + theme_dark() df_pie &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarise(freq = n(), .groups=&#39;drop&#39;) df_pie %&gt;% ggplot( aes( x = &quot;&quot;, y=freq, fill = factor(cut))) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(theta = &quot;y&quot;, start=0) ggplot(data = diamonds)+ geom_bar( mapping = aes(x = cut, fill = cut), show.legend = F, width = 1)+ theme(aspect.ratio = 1)+ labs(x= NULL, y = NULL)+ coord_polar() Claridad diamonds %&gt;% ggplot( aes( y = clarity)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;black&quot;, alpha= 0.7) + geom_text(aes(label = ..count..), stat = &quot;count&quot;, vjust = 1, hjust = 1.5,colour = &quot;blue&quot;)+ scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución claridad&quot;) + theme_get() 4.5.3 Análisis multivariado Precio vs Calidad del corte diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_jitter(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_boxplot(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5)+ facet_wrap(~cut, ncol = 1) diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + geom_smooth() diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + facet_wrap(~clarity)+ geom_smooth() Otro tipo de gráficas interactivas library(stringr) fun_mean &lt;- function(x){ mean &lt;- data.frame( y = mean(x), label = mean(x, na.rm = T) ) return(mean) } means &lt;- diamonds %&gt;% group_by(clarity) %&gt;% summarise(price = round(mean(price), 1)) plot &lt;- diamonds %&gt;% ggplot(aes(x = clarity, y = price)) + geom_boxplot(aes(fill = clarity)) + stat_summary( fun = mean, geom = &quot;point&quot;, colour = &quot;darkred&quot;, shape = 18, size = 3 ) + geom_text( data = means, aes(label = str_c(&quot;$&quot;,price), y = price + 600) ) + ggtitle(&quot;Precio vs Claridad de diamantes&quot;) + xlab(&quot;Claridad&quot;) + ylab(&quot;Precio&quot;) plotly::ggplotly(plot) 4.5.3.1 Graficas para AMES (ejemplos) library(tidymodels) data(ames) ames %&gt;% ggplot(aes(x = Sale_Price)) + ggtitle(&quot;Distribución de Precio de venta&quot;) + ylab(&quot;Conteo&quot;) + xlab(&quot;Condición general&quot;) + geom_histogram( aes(y = ..density..), col = &quot;black&quot;, fill = &quot;red&quot;, bins = 30 ) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1) ames %&gt;% ggplot(aes(x = Overall_Cond)) + geom_bar(fill = &quot;purple&quot;, col = &quot;black&quot;) + geom_text(stat=&#39;count&#39;, aes(label=..count..), vjust=-1) + ggtitle(&quot;Conteo de casas por condición general de calidad&quot;) + ylab(&quot;Conteo&quot;) + xlab(&quot;Condición general&quot;) ames %&gt;% filter(Sale_Condition == &quot;Normal&quot;) %&gt;% ggplot(data = ., aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(size = 1, alpha = 0.2) + ggtitle(&quot;Análisis entre tamaño de vivienda y precio de venta&quot;, subtitle = &quot;Diagrama de dispersión&quot;) + xlab(&quot;Área habitable&quot;) + ylab(&quot;Precio de venta&quot;) + geom_smooth() + scale_x_continuous(limits = c(500, 4000)) ggplot(data = filter(ames, Sale_Condition == &quot;Normal&quot;), aes(x = Total_Bsmt_SF, y = Sale_Price)) + geom_point(size = 1, alpha = 0.2) + ggtitle(&quot;Análisis entre ft2 y precio de venta&quot;, subtitle = &quot;Diagrama de dispersión&quot;) + xlab(&quot;Pies cuadrados&quot;) + ylab(&quot;Precio de venta&quot;) + geom_smooth() + scale_x_continuous(limits = c(500, 2500)) + facet_wrap(~Overall_Cond) ames %&gt;% ggplot(aes(x = reorder(Neighborhood, Sale_Price, median), y = Sale_Price)) + geom_boxplot() + geom_jitter(alpha = 0.1, size = 1) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + ggtitle(&quot;Análisis de precio de venta y vecindario&quot;) + xlab(&quot;Vecindario&quot;) + ylab(&quot;Precio de venta&quot;) ggplot(data = ames, aes(x = reorder(Neighborhood, Sale_Price, median), y = Sale_Price)) + geom_boxplot()+ ggtitle(&quot;Análisis de precio de venta, vecindario por condición&quot;) + xlab(&quot;Vecindario&quot;) + ylab(&quot;Precio de venta&quot;) + facet_wrap(~Overall_Cond) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 4.6 Referencias: What is Exploratory Data Analysis? What is EDA? Diagrama BoxPlot Ggplot2: Elegant Graphics for Data Analysis Plotly "],["machine-learning-conceptos-básicos.html", "Capítulo 5 MACHINE LEARNING: CONCEPTOS BÁSICOS 5.1 ML y Algoritmos 5.2 Análisis Supervisado vs No supervisado 5.3 Sesgo vs varianza 5.4 Pre-procesamiento e ingeniería de datos 5.5 Partición de datos", " Capítulo 5 MACHINE LEARNING: CONCEPTOS BÁSICOS 5.1 ML y Algoritmos Como se habia mencionado, el Machine Learning es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos para hacer predicciones. Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma. El término se utilizó por primera vez en 1959. Sin embargo, ha ganado relevancia en los últimos años debido al aumento de la capacidad de computación y al BOOM de los datos. Un algoritmo para computadoras puede ser pensado como una receta. Describe exactamente qué pasos se realizan uno tras otro. Los ordenadores no entienden las recetas de cocina, sino los lenguajes de programación: En ellos, el algoritmo se descompone en pasos formales (comandos) que el ordenador puede entender. Algunos problemas pueden formularse fácilmente como un algoritmo, por ejemplo, contando del 1 al 100 o comprobando si un número es un número primo. Para otros problemas, esto es muy difícil, por ejemplo, reconocer la escritura o el texto de las teclas. Aquí los procedimientos de aprendizaje de la máquina ayudan. Durante mucho tiempo se han desarrollado algoritmos que permiten analizar los datos existentes y aplicar los conocimientos derivados de ello a los nuevos datos. La cuestión no es solo saber para qué sirve el Machine Learning, sino que saber cómo funciona y cómo poder implementarlo en la industria para aprovecharse de sus beneficios. Hay ciertos pasos que usualmente se siguen para crear un modelo de Machine Learning. Estos son típicamente realizados por científicos de los datos que trabajan en estrecha colaboración con los profesionales de los negocios para los que se está desarrollando el modelo. Seleccionar y preparar un conjunto de datos de entrenamiento Los datos de entrenamiento son un conjunto de datos representativos de los datos que el modelo de Machine Learning ingerirá para resolver el problema que está diseñado para resolver. Los datos de entrenamiento deben prepararse adecuadamente: aleatorizados y comprobados en busca de desequilibrios o sesgos que puedan afectar al entrenamiento. También deben dividirse en dos subconjuntos: el subconjunto de entrenamiento, que se utilizará para entrenar el algoritmo, y el subconjunto de validación, que se utilizará para probarlo y perfeccionarlo. Elegir un algoritmo para ejecutarlo en el conjunto de datos de entrenamiento Este es uno de los pasos más importantes, ya que se debe elegir qué algoritmo utilizar, siendo este un conjunto de pasos de procesamiento estadístico. El tipo de algoritmo depende del tipo (supervisado o no supervisado), la cantidad de datos del conjunto de datos de entrenamiento y del tipo de problema que se debe resolver. Entrenamiento del algoritmo para crear el modelo El entrenamiento del algoritmo es un proceso iterativo: implica ejecutar las variables a través del algoritmo, comparar el resultado con los resultados que debería haber producido, ajustar los pesos y los sesgos dentro del algoritmo que podrían dar un resultado más exacto, y ejecutar las variables de nuevo hasta que el algoritmo devuelva el resultado correcto la mayoría de las veces. El algoritmo resultante, entrenado y preciso, es el modelo de Machine Learning. Usar y mejorar el modelo El paso final es utilizar el modelo con nuevos datos y, en el mejor de los casos, para que mejore en precisión y eficacia con el tiempo. De dónde procedan los nuevos datos dependerá del problema que se resuelva. Por ejemplo, un modelo de Machine Learning diseñado para identificar el spam ingerirá mensajes de correo electrónico, mientras que un modelo de Machine Learning que maneja una aspiradora robot ingerirá datos que resulten de la interacción en el mundo real con muebles movidos o nuevos objetos en la habitación. 5.2 Análisis Supervisado vs No supervisado Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: Aprendizaje supervisado: estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Aprendizaje no supervisado: en el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. En el aprendizaje no supervisado, carecemos de este tipo de etiqueta. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir qué es qué por nosotros mismos. Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad Aprendizaje por refuerzo: su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN 5.2.1 Regresión vs clasificación Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: 5.2.1.1 Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. 5.2.1.2 Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 5.3 Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobreajuste y falta de ajuste. 5.3.0.1 Errores reducibles Error por sesgo: Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos. Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como underfitting. Error por varianza: Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro. Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo. 5.3.0.2 Error irreducible El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 5.3.0.3 Balance entre sesgo y varianza o Trade-off El objetivo de cualquier algoritmo supervisado de Machine Learning es lograr un bias bajo, una baja varianza y a su vez el algoritmo debe lograr un buen rendimiento de predicción. El bias frente a la varianza se refiere a la precisión frente a la consistencia de los modelos entrenados por su algoritmo. Podemos diagnosticarlos de la siguiente manera: Los algoritmos de baja varianza (alto bias) tienden a ser menos complejos, con una estructura subyacente simple o rígida. Los algoritmos de bajo bias (alta varianza) tienden a ser más complejos, con una estructura subyacente flexible. No hay escapatoria a la relación entre el bias y la varianza en Machine Learning, aumentar el bias disminuirá la varianza, aumentar la varianza disminuirá el bias. 5.3.0.4 Error total Comprender el sesgo y la varianza es fundamental para comprender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el error general, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el sesgo es equivalente a la reducción en la varianza. Para construir un buen modelo, necesitamos encontrar un buen equilibrio entre el bias y la varianza de manera que minimice el error total. Un equilibrio óptimo de bias y varianza nunca sobreequiparía o no sería adecuado para el modelo. Por lo tanto comprender el bias y la varianza es fundamental para comprender el comportamiento de los modelos de predicción. 5.3.0.5 Overfitting El modelo es muy particular. Error debido a la varianza Durante el entrenamiento tiene un desempeño muy bueno, pero al pasar nuevos datos su desempeño es malo. 5.3.0.6 Underfitting El modelo es demasiado general. Error debido al sesgo. Durante el entrenamiento no tiene un buen desempeño. 5.4 Pre-procesamiento e ingeniería de datos Hay varios pasos que se deben de seguir para crear un modelo útil: Recopilación de datos. Limpieza de datos. Creación de nuevas variables. Estimación de parámetros. Selección y ajuste del modelo. Evaluación del rendimiento. Al comienzo de un proyecto, generalmente hay un conjunto finito de datos disponibles para todas estas tareas. OJO: A medida que los datos se reutilizan para múltiples tareas, aumentan los riesgos de agregar sesgos o grandes efectos de errores metodológicos. 5.4.1 Pre-procesamiento de datos Como punto de partida para nuestro flujo de trabajo de aprendizaje automático, necesitaremos datos de entrada. En la mayoría de los casos, estos datos se cargarán y almacenarán en forma de data frames o tibbles en R. Incluirán una o varias variables predictoras y, en caso de aprendizaje supervisado, también incluirán un resultado conocido. Sin embargo, no todos los modelos pueden lidiar con diferentes problemas de datos y, a menudo, necesitamos transformar los datos para obtener el mejor rendimiento posible del modelo. Este proceso se denomina pre-procesamiento y puede incluir una amplia gama de pasos, como: Dicotomización de variables: Variables cualitativas que solo pueden tomar el valor \\(0\\) o \\(1\\) para indicar la ausencia o presencia de una condición específica. Estas variables se utilizan para clasificar los datos en categorías mutuamente excluyentes o para activar comandos de encendido / apagado Near Zero Value (nzv) o Varianza Cero: En algunas situaciones, el mecanismo de generación de datos puede crear predictores que solo tienen un valor único (es decir, un predictor de varianza cercando a cero). Para muchos modelos (excluidos los modelos basados en árboles), esto puede hacer que el modelo se bloquee o que el ajuste sea inestable. De manera similar, los predictores pueden tener solo una pequeña cantidad de valores únicos que ocurren con frecuencias muy bajas. Imputaciones: Si faltan algunos predictores, ¿deberían estimarse mediante imputación? * Des-correlacionar: Si hay predictores correlacionados, ¿debería mitigarse esta correlación? Esto podría significar filtrar predictores, usar análisis de componentes principales o una técnica basada en modelos (por ejemplo, regularización). * Normalizar: ¿Deben centrarse y escalar los predictores? Transformar: ¿Es útil transformar los predictores para que sean más simétricos? (por ejemplo, escala logarítmica). Dependiendo del caso de uso, algunos pasos de pre-procesamiento pueden ser indispensables para pasos posteriores, mientras que otros solo son opcionales. Sin embargo, dependiendo de los pasos de pre-procesamiento elegidos, el rendimiento del modelo puede cambiar significativamente en pasos posteriores. Por lo tanto, es muy común probar varias configuraciones. En la tabla, \\(\\checkmark\\) indica que el método es obligatorio para el modelo y \\(\\times\\) indica que no lo es. El símbolo \\(\\circ\\) significa que la técnica puede ayudar al modelo, pero no es obligatorio. Notación: Es posible que la des-correlación de predictores no ayude a mejorar el rendimiento. Sin embargo, menos predictores correlacionados pueden mejorar la estimación de las puntuaciones de importancia de la varianza. Esencialmente, la selección de predictores altamente correlacionados es casi aleatoria. La notación \\(+\\) significa que la respuesta depende de la implementación: Teoricamente, cualquier modelo basado en árboles no requiere imputación de datos, sin embargo, muchas implementaciones de conjuntos de árboles requieren imputación. Si bien los métodos de refuerzo basados en árboles generalmente no requieren la creación de variables ficticias, los modelos que usan xgboost sí lo hacen. 5.4.2 Ingeniería de datos La ingeniería de datos abarca actividades que reformatean los valores de los predictores para que se puedan utilizar de manera eficaz para nuestro modelo. Esto incluye transformaciones y codificaciones de los datos para representar mejor sus características importantes. Por ejemplo: 1.- Supongamos que un conjunto de datos tiene dos predictores que se pueden representar de manera más eficaz en nuestro modelo como una proporción, así, tendríamos un nuevo predictor a partir de la proporción de los dos predictores originales. x x_prop 691 0.1836789 639 0.1698565 969 0.2575758 955 0.2538543 508 0.1350346 2.- Al elegir cómo codificar nuestros datos en el modelado, podríamos elegir una opción que creemos que está más asociada con el resultado. El formato original de los datos, por ejemplo numérico (edad) versus categórico (grupo). Edad Grupo 7 Niños 78 Adultos mayores 17 Adolescentes 25 Adultos 90 Adultos mayores La ingeniería y el pre-procesamiento de datos también pueden implicar el reformateo requerido por el modelo. Algunos modelos utilizan métricas de distancia geométrica y, en consecuencia, los predictores numéricos deben centrarse y escalar para que estén todos en las mismas unidades. De lo contrario, los valores de distancia estarían sesgados por la escala de cada columna. 5.4.3 Recetas Una receta es un objeto que define una serie de pasos para el procesamiento de datos. A diferencia del método de fórmula dentro de una función de modelado, la receta define los pasos sin ejecutarlos inmediatamente; es sólo una especificación de lo que se debe hacer. Como ejemplo utilizaremos un subconjunto de predictores disponibles en los datos de vivienda Ames: Vecindario (cualitativa, con 29 vecindarios en el subconjunto, nombre: neighborhood ) La superficie habitable bruta sobre el nivel del suelo (continua, nombre: Gr_Liv_Area) Año de constriccuón (nombre: Year_Built) Tipo de edificio (nombre: Bldg_Type con los valores: OneFam \\(= 1,924\\) , TwoFmCon \\(=46\\), Duplex \\(=95\\), Twnhs \\(=80\\), and TwnhsE \\(=197\\)) \\(\\dots\\) Un modelo de regresión lineal ordinario inicial se ajusta a estos datos con la función estándar lm() de la siguiente manera: lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames) Cuando se ejecuta esta función, los datos se convierten en a una matriz de diseño numérico (también llamada matriz de modelo) y luego se utiliza el método de mínimos cuadrados para estimar los parámetros. Lo que hace la fórmula anterior se puede descomponer en una serie de pasos: 1.- El precio de venta se define como el resultado, mientras que las variables de vecindario, superficie habitable bruta, año de construcción y tipo de edificio se definen como predictores. 2.- Se aplica una transformación logarítmica al predictor de superficie habitable bruta. 3.- Las columnas de vecindad y tipo de edificio se convierten de un formato no numérico a un formato numérico (dado que los mínimos cuadrados requieren predictores numéricos). La siguiente receta es equivalente a la fórmula anterior: simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal_predictors()) simple_ames ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Log transformation on Gr_Liv_Area ## Dummy variables from all_nominal_predictors() Analicemos esto: La función recipe() captura los roles de las variables, (por ejemplo, predictor, resultado). Solo ustiliza los datos ames para determinar los tipos de datos para las columnas. step_log() declara que la variable Gr_Liv_Area debe transformarse en logaritmo. step_dummy() se usa para especificar qué variables deben convertirse de un formato cualitativo a un formato cuantitativo, en este caso, usando variables ficticias o indicadoras. La función all_nominal_predictors() captura los nombres de las columnas predictoras que actualmente son de carácter factor. Ventajas de usar una receta: Los cálculos se pueden reciclar entre modelos ya que no están estrechamente acoplados a la función de modelado. Una receta permite un conjunto más amplio de opciones de procesamiento de datos que las que pueden ofrecer las fórmulas. La sintaxis puede ser muy compacta. Por ejemplo, all_nominal_predictors() se puede usar para capturar muchas variables para tipos específicos de procesamiento, mientras que una fórmula requeriría que cada una se enumere explícitamente. Todo el procesamiento de datos se puede capturar en un solo objeto en lugar de tener scripts que se repiten o incluso se distribuyen en diferentes archivos. 5.4.3.1 Pasos de recetas 5.4.3.1.1 Codificación de datos cualitativos en formato numérico Una de las tareas de ingeniería de datos más comunes es transformar datos nominales o cualitativos (factores o caracteres) para que puedan codificarse o representarse numéricamente. A veces, podemos alterar los niveles de factores de una columna cualitativa de manera útil antes de tal transformación. Por ejemplo: step_unknown() cambia los valores perdidos en un nivel de factor desconocido. step_novel() puede asignar un nuevo nivel si anticipamos que se puede encontrar un nuevo factor en datos futuros. step_other() analiza las frecuencias de los niveles de los factores en el conjunto de datos y convierte los valores que ocurren con poca frecuencia a un nivel general de otro, con un umbral que se puede especificar. Un buen ejemplo es el predictor de vecindad en nuestros datos: ggplot(ames, aes(y = Neighborhood)) + geom_bar() + labs(y = NULL) Aquí hay dos vecindarios que tienen menos de cinco propiedades; en este caso, no se incluyó ninguna casa en el vecindario Landmark. Para algunos modelos, puede resultar problemático tener variables ficticias con una sola entrada distinta de cero en la columna. Como mínimo, es muy improbable que estas características sean importantes para un modelo. Si agregamos step_other (Neighborhood, threshold = 0.01) a nuestra receta, el último \\(1\\%\\) de los vecindarios se agrupará en un nuevo nivel llamado otro, esto atrapará a 8 vecindarios. simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% step_dummy(all_nominal_predictors()) 5.4.3.1.2 Interacciones Los efectos de interacción involucran dos o más predictores. Tal efecto ocurre cuando un predictor tiene un efecto sobre el resultado que depende de uno o más predictores. Numéricamente, un término de interacción entre predictores se codifica como su producto. Las interacciones solo se definen en términos de su efecto sobre el resultado y pueden ser combinaciones de diferentes tipos de datos (por ejemplo, numéricos, categóricos, etc.). Después de explorar el conjunto de datos de Ames, podríamos encontrar que las pendientes de regresión para el área habitable bruta difieren para los diferentes tipos de edificios: ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = .2) + facet_wrap(~ Bldg_Type) + geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = &quot;red&quot;) + scale_x_log10() + scale_y_log10() + labs(x = &quot;Gross Living Area&quot;, y = &quot;Sale Price (USD)&quot;) Con la receta actual, step_dummy() ya ha creado variables ficticias. ¿Cómo combinaríamos estos para una interacción? El paso adicional se vería como step_interact(~ términos de interacción) donde los términos en el lado derecho de la tilde son las interacciones. Estos pueden incluir selectores, por lo que sería apropiado usar: simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% # Gr_Liv_Area está en escala logarítmica step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) Se pueden especificar interacciones adicionales en esta fórmula separándolas con el signo \\(+\\). La receta solamente utilizará interacciones entre diferentes variables. 5.4.3.1.3 Transformaciones generales Reflejando las operaciones originales de dplyr, los siguientes pasos se pueden usar para realizar una variedad de operaciones básicas a los datos. step_select(): Selecciona un subconjunto de variables específicas en el conjunto de datos. step_mutate(): Crea una nueva variable o modifica una existente usando dplyr::mutate(). step_mutate_at(): Lee una especificación de un paso de receta que modificará las variables seleccionadas usando una función común a través de dplyr::mutate_at(). step_filter(): Crea una especificación de un paso de receta que eliminará filas usando dplyr::filter(). step_arrange(): Ordena el conjunto de datos de acuerdo con una o más variables. step_rm(): Crea una especificación de un paso de receta que eliminará las variables según su nombre, tipo o función. step_nzv(): Realiza una selección de variables eliminando todas aquellas cuya varianza se encuentre cercana a cero. step_naomit(): Elimina todos los renglones que tengan alguna variable con valores perdidos. step_normalize(): Centra y escala las variables numéricas especificadas, generando una transformación a una distribución normal estándar. step_range(): Transforma el rango de un conjunto de variables numéricas al especificado. step_interact(): Crea un nuevo conjunto de variables basadas en la interacción entre dos variables. step_ratio(): Crea una nueva variable a partir del cociente entre dos variables. all_predictors(): Selecciona a todos los predictores del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_numeric_predictors(): Selecciona a todos los predictores numéricos del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_nominal_predictors(): Selecciona a todos los predictores nominales del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. receta &lt;- recipe( ~ ., data = iris) %&gt;% step_mutate( dbl_width = Sepal.Width * 2, half_length = Sepal.Length / 2 ) %&gt;% step_normalize(all_numeric_predictors(), na_rm = T) %&gt;% step_dummy(all_nominal_predictors(), min_unique = 5) receta ## Data Recipe ## ## Inputs: ## ## role #variables ## predictor 5 ## ## Operations: ## ## Variable mutation for dbl_width, half_length ## Centering and scaling for all_numeric_predictors() ## Dummy variables from all_nominal_predictors(), 5 La guía completa de las familia de funciones step puede consultarse en la documentación oficial 5.4.3.2 Ejecutar el pre-procesamiento de datos 5.4.3.2.1 Preparar la receta prep() Esta función devuelve una receta actualizada con las estimaciones. Dado un conjunto de datos, la función prep() estima las cantidades requeridas y las estadísticas necesarias para cualquier paso declarado en la receta. Los datos faltantes se manejan en los pasos; no hay una opción na.rm a nivel de receta o en prep(). prep &lt;- prep(simple_ames) prep ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Log transformation on Gr_Liv_Area [trained] ## Collapsing factor levels for Neighborhood [trained] ## Dummy variables from Neighborhood, Bldg_Type [trained] ## Interactions with Gr_Liv_Area:(Bldg_Type_TwoFmCon + Bldg_Type_Duplex + Bldg_Type_Twnhs + Bldg_Type_TwnhsE) [trained] 5.4.3.2.2 Extracción de datos bake La función bake() toma una receta como entrenada (con la función prep()) y aplica las operaciones a un conjunto de datos para crear una matriz de diseño. Si el conjunto de datos no es demasiado grande, se puede ahorrar tiempo usando la opción retain = TRUE de prep(). Esto almacena la versión procesada del conjunto de datos al que se le aplicó la receta. Así, con esta opción configurada (por default), la función bake (object, new_data = NULL) devolverá los datos con los que se entrenó la receta. Nota: La función juice() devolverá los resultados de una receta en la que se hayan aplicado todos los pasos a los datos. Similar a la función bake() con el comando new_data = NULL. bake(prep, new_data = NULL) %&gt;% glimpse() ## Rows: 2,930 ## Columns: 32 ## $ Gr_Liv_Area &lt;dbl&gt; 3.219060, 2.95230~ ## $ Year_Built &lt;int&gt; 1960, 1961, 1958,~ ## $ Sale_Price &lt;int&gt; 215000, 105000, 1~ ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 0, 0, 0, 1, 1,~ ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Northwest_Ames &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Sawyer_West &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Mitchell &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Brookside &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Crawford &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Timberland &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Northridge &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Stone_Brook &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Clear_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Meadow_Village &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_Briardale &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Neighborhood_other &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Gr_Liv_Area_x_Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Gr_Liv_Area_x_Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Gr_Liv_Area_x_Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~ ## $ Gr_Liv_Area_x_Bldg_Type_TwnhsE &lt;dbl&gt; 0.000000, 0.00000~ 5.5 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 5.5.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. Supongamos que asignamos el \\(80\\%\\) de los datos al conjunto de entrenamiento y el \\(20\\%\\) restante a las pruebas. El método más común es utilizar un muestreo aleatorio simple. El paquete rsample tiene herramientas para realizar divisiones de datos como esta; la función initial_split() fue creada para este propósito. library(tidymodels) tidymodels_prefer() # Fijar un número aleatorio con para que los resultados puedan ser reproducibles set.seed(123) # Partición 80/20 de los datos ames_split &lt;- initial_split(ames, prop = 0.80) ames_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2344/586/2930&gt; La información impresa denota la cantidad de datos en el conjunto de entrenamiento \\((n = 2,344)\\), la cantidad en el conjunto de prueba \\((n = 586)\\) y el tamaño del grupo original de muestras \\((n = 2,930)\\). El objeto ames_split es un objeto rsplit y solo contiene la información de partición; para obtener los conjuntos de datos resultantes, aplicamos dos funciones más: ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) dim(ames_train) ## [1] 2344 74 El muestreo aleatorio simple es apropiado en muchos casos, pero hay excepciones. Cuando hay un desbalance de clases en los problemas de clasificación, el uso de una muestra aleatoria simple puede asignar al azar estas muestras poco frecuentes de manera desproporcionada al conjunto de entrenamiento o prueba. Para evitar esto, se puede utilizar un muestreo estratificado. La división de entrenamiento/prueba se lleva a cabo por separado dentro de cada clase y luego estas submuestras se combinan en el conjunto general de entrenamiento y prueba. Para los problemas de regresión, los datos de los resultados se pueden agrupar artificialmente en cuartiles y luego realizar un muestreo estratificado cuatro veces por separado. Este es un método eficaz para mantener similares las distribuciones del resultado entre el conjunto de entrenamiento y prueba. Observamos que la distribución del precio de venta está sesgada a la derecha. Las casas más caras no estarían bien representadas en el conjunto de entrenamiento con una simple partición; esto aumentaría el riesgo de que nuestro modelo sea ineficaz para predecir el precio de dichas propiedades. Las líneas verticales punteadas indican los cuatro cuartiles para estos datos. Una muestra aleatoria estratificada llevaría a cabo la división 80/20 dentro de cada uno de estos subconjuntos de datos y luego combinaría los resultados. En rsample, esto se logra usando el argumento de estratos: set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Hay muy pocas desventajas en el uso de muestreo estratificado. Un caso es cuando los datos tienen un componente de tiempo, como los datos de series de tiempo. Aquí, es más común utilizar los datos más recientes como conjunto de prueba. El paquete rsample contiene una función llamada initial_time_split() que es muy similar a initial_split(). En lugar de usar un muestreo aleatorio, el argumento prop denota qué proporción de la primera parte de los datos debe usarse como conjunto de entrenamiento; la función asume que los datos se han clasificado previamente en un orden apropiado. 5.5.2 ¿Qué proporción debería ser usada? No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Muy pocos datos en el conjunto de entrenamiento obstaculizan la capacidad del modelo para encontrar estimaciones de parámetros adecuadas y muy pocos datos en el conjunto de prueba reducen la calidad de las estimaciones de rendimiento. Se debe elegir un porcentaje que cumpla con los objetivos de nuestro proyecto con consideraciones que incluyen: Costo computacional en el entrenamiento del modelo. Costo computacional en la evaluación del modelo. Representatividad del conjunto de formación. Representatividad del conjunto de pruebas. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 5.5.3 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobreajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Los conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una sola partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo. Con rsample, un conjunto de validación es como cualquier otro objeto de remuestreo; este tipo es diferente solo en que tiene una sola iteración set.seed(12) val_set &lt;- validation_split(ames_train, prop = 3/4, strata = NULL) val_set #val_set contiene el conjunto de entrenamiento y validación. ## # Validation Set Split (0.75/0.25) ## # A tibble: 1 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1756/586]&gt; validation Esta función regresa una columna para los objetos de división de datos y una columna llamada id que tiene una cadena de caracteres con el identificador de remuestreo. El argumento de estratos hace que el muestreo aleatorio se lleve a cabo dentro de la variable de estratificación. Esto puede ayudar a garantizar que el número de datos en los datos del análisis sea equivalente a las proporciones del conjunto de datos original. (Los estratos inferiores al 10% del total se agrupan). 5.5.4 Leave-one-out cross-validation La validación cruzada es una manera de predecir el ajuste de un modelo a un hipotético conjunto de datos de prueba cuando no disponemos del conjunto explícito de datos de prueba. El método LOOCV en un método iterativo que se inicia empleando como conjunto de entrenamiento todas las observaciones disponibles excepto una, que se excluye para emplearla como validación. Si se emplea una única observación para calcular el error, este varía mucho dependiendo de qué observación se haya seleccionado. Para evitarlo, el proceso se repite tantas veces como observaciones disponibles se tengan, excluyendo en cada iteración una observación distinta, ajustando el modelo con el resto y calculando el error con dicha observación. Finalmente, el error estimado por el es el promedio de todos lo \\(i\\) errores calculados. La principal desventaja de este método es su costo computacional. El proceso requiere que el modelo sea reajustado y validado tantas veces como observaciones disponibles se tengan lo que en algunos casos puede ser muy complicado. rsample contiene la función loo_cv(). set.seed(55) ames_loo &lt;- loo_cv(ames_train) ames_loo ## # Leave-one-out cross-validation ## # A tibble: 2,342 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2341/1]&gt; Resample1 ## 2 &lt;split [2341/1]&gt; Resample2 ## 3 &lt;split [2341/1]&gt; Resample3 ## 4 &lt;split [2341/1]&gt; Resample4 ## 5 &lt;split [2341/1]&gt; Resample5 ## 6 &lt;split [2341/1]&gt; Resample6 ## 7 &lt;split [2341/1]&gt; Resample7 ## 8 &lt;split [2341/1]&gt; Resample8 ## 9 &lt;split [2341/1]&gt; Resample9 ## 10 &lt;split [2341/1]&gt; Resample10 ## # ... with 2,332 more rows 5.5.4.1 Cálculo del error En la validación cruzada dejando uno fuera se realizan tantas iteraciones como muestras \\((N)\\) tenga el conjunto de datos. De forma que para cada una de las \\(N\\) iteraciones se realiza un cálculo de error. El resultado final se obtiene realizando la media de los \\(N\\) errores obtenidos, según la fórmula: \\[E = \\frac{1}{N}\\sum_{i = 1}^N E_i\\] 5.5.5 V Fold Cross Validation En la validación cruzada de V iteraciones (V Fold Cross Validation) los datos de muestra se dividen en V subconjuntos. Uno de los subconjuntos se utiliza como datos de prueba y el resto \\((V-1)\\) como datos de entrenamiento. El proceso de validación cruzada es repetido durante \\(v\\) iteraciones, con cada uno de los posibles subconjuntos de datos de prueba. Finalmente se obtiene el promedio de los rendimientos de cada iteración para obtener un único resultado. Lo más común es utilizar la validación cruzada de 10 iteraciones. Este método de validación cruzada se utiliza principalmente para: Estimar el error cuando nuestro conjunto de prueba es muy pequeño. Es decir, se tiene la misma confuguración de parámetos y solamente cambia el conjunto de prueba y validación. Encontrar lo mejores hiperparámetros que ajusten mejor el modelo. Es decir, en cada bloque se tiene una configuración de hiperparámetros distinto y se seleccionará aquellos hiperparámetros que hayan producido el error más pequeño. En la función vfold_cv() la entrada principal es el conjunto de entrenamiento, así como el número de bloques: set.seed(55) ames_folds &lt;- vfold_cv(ames_train, v = 10) ames_folds ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2107/235]&gt; Fold01 ## 2 &lt;split [2107/235]&gt; Fold02 ## 3 &lt;split [2108/234]&gt; Fold03 ## 4 &lt;split [2108/234]&gt; Fold04 ## 5 &lt;split [2108/234]&gt; Fold05 ## 6 &lt;split [2108/234]&gt; Fold06 ## 7 &lt;split [2108/234]&gt; Fold07 ## 8 &lt;split [2108/234]&gt; Fold08 ## 9 &lt;split [2108/234]&gt; Fold09 ## 10 &lt;split [2108/234]&gt; Fold10 La columna denominada splits contiene la información sobre cómo dividir los datos (similar al objeto utilizado para crear la partición inicial de entrenamiento / prueba). Si bien cada fila de divisiones tiene una copia incrustada de todo el conjunto de entrenamiento, R es lo suficientemente inteligente como para no hacer copias de los datos en la memoria. El método de impresión dentro del tibble muestra la frecuencia de cada uno: [2K / 230] indica que aproximadamente dos mil muestras están en el conjunto de análisis y 230 están en ese conjunto de evaluación en particular. Estos objetos rsample también contienen siempre una columna de caracteres llamada id que etiqueta la partición. Algunos métodos de remuestreo requieren varios campos de identificación. Para recuperar manualmente los datos particionados, las funciones de analysis() y assessment() devuelven los de datos de análisis y evaluación respectivamente. # Primer bloque ames_folds$splits[[1]] %&gt;% analysis() %&gt;% # O assessment() head(7) ## # A tibble: 7 x 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 One_Story_1946_a~ Residential_~ 70 8400 Pave No_All~ Regular ## 2 Two_Story_PUD_19~ Residential_~ 21 1680 Pave No_All~ Regular ## 3 Two_Story_PUD_19~ Residential_~ 21 1680 Pave No_All~ Regular ## 4 Two_Story_PUD_19~ Residential_~ 21 1680 Pave No_All~ Regular ## 5 One_Story_PUD_19~ Residential_~ 53 4043 Pave No_All~ Regular ## 6 One_Story_PUD_19~ Residential_~ 24 2280 Pave No_All~ Regular ## 7 One_Story_PUD_19~ Residential_~ 50 7175 Pave No_All~ Regular ## # ... with 67 more variables: Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, ## # Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, ## # Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;, ## # Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, ## # Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, ## # Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;, ... 5.5.6 Medidas de ajuste Las medidas de ajuste obtenidas pueden ser utilizadas para estimar cualquier medida cuantitativa de ajuste apropiada para los datos y el modelo. En un modelo basado en clasificación binaria, para resumir el ajuste del modelo se pueden usar las medidas: Tasa de error de clasificación (Accuracy) Precisión Sensibilidad o coertura (Recall) Especificidad Cuando el valor a predecir se distribuye de forma continua se puede calcular el error utilizando medidas como: Error porcentual absoluto medio (MAPE) Error absoluto medio (MAE) Error cuadrático medio (MSE) Raíz del error cuadrático medio (RMSE) Raíz del error logarítmico cuadrático medio (RMLSE) \\(R^2\\) (Coeficiente de determinación) \\(R^2_a\\) (Coeficiente de determinación ajustado) 5.5.6.1 Cálculo del error En cada una de las \\(v\\) iteraciones de este tipo de validación se realiza un cálculo de error. El resultado final lo obtenemos a partir de realizar la media de los \\(V\\) valores de errores obtenidos, según la fórmula: \\[E = \\frac{1}{V}\\sum_{i = 1}^vE_i\\] 5.5.7 Validación cruzada para series de tiempo En este procedimiento, hay una serie de conjuntos de prueba, cada uno de los cuales consta de una única observación. El conjunto de entrenamiento correspondiente consta solo de observaciones que ocurrieron antes de la observación que forma el conjunto de prueba. Por lo tanto, no se pueden utilizar observaciones futuras para construir el pronóstico. El siguiente diagrama ilustra la serie de conjuntos de entrenamiento y prueba, donde las observaciones azules forman los conjuntos de entrenamiento y las observaciones rojas forman los conjuntos de prueba. La precisión del pronóstico se calcula promediando los conjuntos de prueba. Este procedimiento a veces se conoce como evaluación en un origen de pronóstico continuo porque el origen en el que se basa el pronóstico avanza en el tiempo. Con los pronósticos de series de tiempo, los pronósticos de un paso pueden no ser tan relevantes como los pronósticos de varios pasos. En este caso, el procedimiento de validación cruzada basado en un origen de pronóstico continuo se puede modificar para permitir el uso de errores de varios pasos. Suponga que estamos interesados en modelos que producen buenos pronósticos de 4 pasos por delante. Entonces el diagrama correspondiente se muestra a continuación. La validación cruzada de series de tiempo se implementa con la función tsCV() del paquete forecast. En el siguiente ejemplo, comparamos el RMSE residual con el RMSE obtenido mediante la validación cruzada de series de tiempo. library(fpp) e &lt;- tsCV(dj, rwf, drift=TRUE, h=1) sqrt(mean(e^2, na.rm=TRUE)) ## [1] 22.68249 sqrt(mean(residuals(rwf(dj, drift=TRUE))^2, na.rm=TRUE)) ## [1] 22.49681 Aquí se aplicó una caminata aleatoria con deriva a la serie temporal del índice Dow-Jones. El primer cálculo implementa una validación cruzada de series de tiempo de un solo paso en la que el parámetro de deriva se vuelve a estimar en cada origen de pronóstico. El segundo cálculo estima el parámetro de deriva una vez para todo el conjunto de datos y luego calcula el RMSE a partir de los pronósticos de un paso. Como se esperaba, el RMSE de los residuos es menor, ya que los pronósticos correspondientes se basan en un modelo ajustado a todo el conjunto de datos, en lugar de ser pronósticos verdaderos. La función tsCV() es muy general y funcionará para cualquier función de pronóstico que devuelva un objeto de clase forecast. Ni siquiera se tiene que especificar el tamaño de muestra mínimo para el ajuste del modelo, ya que se ajustará silenciosamente a los modelos comenzando con una sola observación y devolverá un valor faltante cuando el modelo no se pueda estimar. 5.5.8 Otros tipos de validación cruzada Validación cruzada repetida Validación cruzada de Monte Carlo Validación cruzada aleatoria Validación cruzada en \\(V\\) bloques para modelos de autoregresión "],["machine-learning-aprendizaje-supervisado.html", "Capítulo 6 MACHINE LEARNING: APRENDIZAJE SUPERVISADO 6.1 Regresión: Preparación de conjunto de datos 6.2 Clasificación: Preparación de Datos 6.3 Regresión lineal simple y múltiple 6.4 Métodos se selección de variables 6.5 Regresión logística 6.6 Regresión regularizada 6.7 KNN 6.8 Tree decision 6.9 Bagging 6.10 Random forest 6.11 Boosting", " Capítulo 6 MACHINE LEARNING: APRENDIZAJE SUPERVISADO 6.1 Regresión: Preparación de conjunto de datos En esta sección, prepararemos datos para ajustar modelos de regresión y de clasificación, usando la paquetería recipes. Primero ajustaremos la receta, después obtendremos la receta actualizada con las estimaciones y al final el conjunto de datos listo para el modelo. 6.1.1 Datos de regresión: Ames Housing Data Los datos que usaremos son los de Ames Housing Data, el conjunto de datos contiene información de la Ames Assessors Office utilizada para calcular valuaciones para propiedades residenciales individuales vendidas en Ames, IA, de 2006 a 2010. Podemos encontrar más información en el siguiente link Ames Housing Data. library(tidymodels) library(stringr) library(tidyverse) #library(modeldata) Esta paquetería también la carga el paquete tidymodels data(ames) names(ames) ## [1] &quot;MS_SubClass&quot; &quot;MS_Zoning&quot; &quot;Lot_Frontage&quot; ## [4] &quot;Lot_Area&quot; &quot;Street&quot; &quot;Alley&quot; ## [7] &quot;Lot_Shape&quot; &quot;Land_Contour&quot; &quot;Utilities&quot; ## [10] &quot;Lot_Config&quot; &quot;Land_Slope&quot; &quot;Neighborhood&quot; ## [13] &quot;Condition_1&quot; &quot;Condition_2&quot; &quot;Bldg_Type&quot; ## [16] &quot;House_Style&quot; &quot;Overall_Cond&quot; &quot;Year_Built&quot; ## [19] &quot;Year_Remod_Add&quot; &quot;Roof_Style&quot; &quot;Roof_Matl&quot; ## [22] &quot;Exterior_1st&quot; &quot;Exterior_2nd&quot; &quot;Mas_Vnr_Type&quot; ## [25] &quot;Mas_Vnr_Area&quot; &quot;Exter_Cond&quot; &quot;Foundation&quot; ## [28] &quot;Bsmt_Cond&quot; &quot;Bsmt_Exposure&quot; &quot;BsmtFin_Type_1&quot; ## [31] &quot;BsmtFin_SF_1&quot; &quot;BsmtFin_Type_2&quot; &quot;BsmtFin_SF_2&quot; ## [34] &quot;Bsmt_Unf_SF&quot; &quot;Total_Bsmt_SF&quot; &quot;Heating&quot; ## [37] &quot;Heating_QC&quot; &quot;Central_Air&quot; &quot;Electrical&quot; ## [40] &quot;First_Flr_SF&quot; &quot;Second_Flr_SF&quot; &quot;Gr_Liv_Area&quot; ## [43] &quot;Bsmt_Full_Bath&quot; &quot;Bsmt_Half_Bath&quot; &quot;Full_Bath&quot; ## [46] &quot;Half_Bath&quot; &quot;Bedroom_AbvGr&quot; &quot;Kitchen_AbvGr&quot; ## [49] &quot;TotRms_AbvGrd&quot; &quot;Functional&quot; &quot;Fireplaces&quot; ## [52] &quot;Garage_Type&quot; &quot;Garage_Finish&quot; &quot;Garage_Cars&quot; ## [55] &quot;Garage_Area&quot; &quot;Garage_Cond&quot; &quot;Paved_Drive&quot; ## [58] &quot;Wood_Deck_SF&quot; &quot;Open_Porch_SF&quot; &quot;Enclosed_Porch&quot; ## [61] &quot;Three_season_porch&quot; &quot;Screen_Porch&quot; &quot;Pool_Area&quot; ## [64] &quot;Pool_QC&quot; &quot;Fence&quot; &quot;Misc_Feature&quot; ## [67] &quot;Misc_Val&quot; &quot;Mo_Sold&quot; &quot;Year_Sold&quot; ## [70] &quot;Sale_Type&quot; &quot;Sale_Condition&quot; &quot;Sale_Price&quot; ## [73] &quot;Longitude&quot; &quot;Latitude&quot; 6.1.1.1 Separación de los datos El primer paso para crear un modelo de regresión es dividir nuestros datos originales en un conjunto de entrenamiento y prueba. No hay que olvidar usar siempre una semilla con la función set.seed() para que sus resultados sean reproducibles. Primero usaremos la función initial_split() de rsample para dividir los datos ames en conjuntos de entrenamiento y prueba. Usamos el parámetro prop para indicar la proporción de los conjuntos train y test. set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.70) El objeto ames_split es un objeto rsplit y solo contiene la información de partición, para obtener los conjuntos de datos resultantes, aplicamos dos funciones adicionales, training y testing. ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Estos objetos son data frames con las mismas columnas que los datos originales, pero solo las filas apropiadas para cada conjunto. También existe la función vfold_cv que se usa para crear v particiones del conjunto de entrenamiento. set.seed(2453) ames_folds&lt;- vfold_cv(ames_train) Ya con los conjuntos de entrenamiento y prueba definidos, iniciaremos con feature engeeniring sobre el conjunto de entrenamiento. 6.1.1.2 Definición de la receta Ahora usaremos la función vista en la sección anterior, recipe(), para definir los pasos de preprocesamiento antes de usar los datos para modelado. Usamos la función step_mutate() para generar nuevas variables dentro de la receta. La función step_interact() nos ayuda a crear nuevas variables que son interacciones entre las variables especificadas. Con la función step_ratio() creamos proporciones con las variables especificadas. fct_collapse() se usa para recategorizar variables, colapsando categorías de la variable. step_relevel nos ayuda a asignar la categoria deseada de una variable como referencia. step_normalize() es de gran utilidad ya que sirve para normalizar las variables que se le indiquen. step_dummy() Nos ayuda a crear variables One Hot Encoding. Por último usamos la función step_rm() para eliminar variables que no son de utilidad para el modelo. Ahora crearemos algunas variables auxiliares que podrían ser de utilidad para el ajuste de un modelo de regresión, entre ella: Log_SalePrice, la cual será la variable a predecir. Una vez que el modelo haga la predicción del logaritmo del precio, es importante calcular con la función exponencial el precio real. ames_train &lt;- ames_train %&gt;% mutate( Log_SalePrice = log(Sale_Price) ) %&gt;% select(-Sale_Price) ames_test &lt;- ames_test %&gt;% mutate( Log_SalePrice = log(Sale_Price) ) %&gt;% select(-Sale_Price) receta_casas &lt;- recipe(Log_SalePrice ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_unknown(Fence) %&gt;% step_unknown(Garage_Type) %&gt;% step_unknown(Garage_Finish) %&gt;% step_unknown(Garage_Cond) %&gt;% step_unknown(Bsmt_Cond) %&gt;% step_unknown(Bsmt_Exposure) %&gt;% step_unknown(BsmtFin_Type_1) %&gt;% step_unknown(BsmtFin_Type_2) %&gt;% step_unknown(Mas_Vnr_Type) %&gt;% step_unknown(Electrical) %&gt;% step_unknown(Heating_QC) %&gt;% step_unknown(Pool_QC) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_mutate(TotalBaths = Full_Bath + Bsmt_Full_Bath + 0.5 * (Half_Bath + Bsmt_Half_Bath), Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Porch_SF = Enclosed_Porch + ThirdSsn_Porch + Open_Porch_SF, Porch = factor(Porch_SF &gt; 0), Pool = if_else(Pool_Area &gt; 0,1,0) ) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_ratio(Second_Flr_SF, denom = denom_vars(First_Flr_SF)) %&gt;% step_mutate(Exter_Cond = fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;)), Condition_1 = fct_collapse(Condition_1, Artery_Feedr = c(&quot;Feedr&quot;, &quot;Artery&quot;), Railr = c(&quot;RRAn&quot;, &quot;RRNn&quot;, &quot;RRNe&quot;, &quot;RRAe&quot;), Norm = &quot;Norm&quot;, Pos = c(&quot;PosN&quot;, &quot;PosA&quot;)), Land_Slope = fct_collapse(Land_Slope, Mod_Sev = c(&quot;Mod&quot;, &quot;Sev&quot;)), Land_Contour = fct_collapse(Land_Contour, Low_HLS = c(&quot;Low&quot;,&quot;HLS&quot;), Bnk_Lvl = c(&quot;Lvl&quot;,&quot;Bnk&quot;)), Lot_Shape = fct_collapse(Lot_Shape, IRREG = c(&quot;Slightly_Irregular&quot;, &quot;Moderately_Irregular&quot;, &quot;Irregular&quot;)), Bsmt_Cond = fct_collapse(Bsmt_Cond, Exc = c(&quot;Good&quot;, &quot;Excellent&quot;)), BsmtFin_Type_1 = fct_collapse(BsmtFin_Type_1, Rec_BLQ = c(&quot;Rec&quot;, &quot;BLQ&quot;)), BsmtFin_Type_2 = fct_collapse(BsmtFin_Type_2, Rec_BLQ = c(&quot;Rec&quot;, &quot;BLQ&quot;,&quot;LwQ&quot;)), Neighborhood = fct_collapse(Neighborhood, NoRidge_GrnHill = c(&quot;Northridge&quot;, &quot;Green_Hills&quot;), Crawfor_Greens = c(&quot;Crawford&quot;, &quot;Greens&quot;), Blueste_Mitchel = c(&quot;Blueste&quot;, &quot;Mitchell&quot;), Blmngtn_CollgCr = c(&quot;Bloomington_Heights&quot;, &quot;College_Creek&quot;), NPkVill_NAmes = c(&quot;Northpark_Villa&quot;, &quot;North_Ames&quot;), Veenker_StoneBr = c(&quot;Veenker&quot;, &quot;Stone_Brook&quot;), BrDale_IDOTRR = c(&quot;Briardale&quot;, &quot;Iowa_DOT_and_Rail_Road&quot;), SWISU_Sawyer = c(&quot;South_and_West_of_Iowa_State_University&quot;, &quot;Sawyer&quot;), ClearCr_Somerst = c(&quot;Clear_Creek&quot;, &quot;Somerset&quot;)), Heating = fct_collapse(Heating, Grav_Wall = c(&quot;Grav&quot;, &quot;Wall&quot;), GasA_W = c(&quot;GasA&quot;, &quot;GasW&quot;, &quot;OthW&quot;)), MS_Zoning = fct_collapse(MS_Zoning, I_R_M_H = c(&quot;Residential_Medium_Density&quot;, &quot;I_all&quot;, &quot;Residential_High_Density&quot; )), Bldg_Type = fct_collapse(Bldg_Type, Du_Tu = c(&quot;Duplex&quot;, &quot;Twnhs&quot;)), Foundation = fct_collapse(Foundation, Wood_Stone = c(&quot;Wood&quot;, &quot;Stone&quot;)), Functional = fct_collapse(Functional, Min = c(&quot;Min1&quot;, &quot;Min2&quot;), Maj = c(&quot;Maj1&quot;, &quot;Maj2&quot;, &quot;Mod&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_relevel(Condition_1, ref_level = &quot;Norm&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:Bedroom_AbvGr) %&gt;% step_interact(~ TotalSF:TotRms_AbvGrd) %&gt;% step_interact(~ Age_House:TotRms_AbvGrd) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_interact(~ matches(&quot;BsmtFin_Type_1&quot;):BsmtFin_SF_1) %&gt;% step_interact(~ matches(&quot;BsmtFin_Type_1&quot;):Total_Bsmt_SF) %&gt;% step_interact(~ matches(&quot;Heating_QC&quot;):TotRms_AbvGrd) %&gt;% step_interact(~ matches(&quot;Heating_QC&quot;):TotalSF) %&gt;% step_interact(~ matches(&quot;Heating_QC&quot;):Second_Flr_SF) %&gt;% step_interact(~ matches(&quot;Neighborhood&quot;):matches(&quot;Condition_1&quot;)) %&gt;% step_rm(First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Porch_SF, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. casa_juiced &lt;- juice(receta_casas) casa_test_bake &lt;- bake(receta_casas, new_data = ames_test) 6.2 Clasificación: Preparación de Datos Ahora prepararemos los datos para un ejemplo de churn, es decir, la tasa de cancelación de clientes. Usaremos datos de Telco. telco &lt;- read_csv(&quot;data/Churn.csv&quot;) names(telco) ## [1] &quot;customerID&quot; &quot;gender&quot; &quot;SeniorCitizen&quot; &quot;Partner&quot; ## [5] &quot;Dependents&quot; &quot;tenure&quot; &quot;PhoneService&quot; &quot;MultipleLines&quot; ## [9] &quot;InternetService&quot; &quot;OnlineSecurity&quot; &quot;OnlineBackup&quot; &quot;DeviceProtection&quot; ## [13] &quot;TechSupport&quot; &quot;StreamingTV&quot; &quot;StreamingMovies&quot; &quot;Contract&quot; ## [17] &quot;PaperlessBilling&quot; &quot;PaymentMethod&quot; &quot;MonthlyCharges&quot; &quot;TotalCharges&quot; ## [21] &quot;Churn&quot; Como en el ejemplo de regresión, primero crearemos los conjuntos de entrenamiento y de prueba. set.seed(1234) telco_split &lt;- initial_split(telco, prop = .7) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) En el siguiente chunk definiremos la receta con funciones usadas en el ejemplo anterior más la función step_num2factor() que nos ayuda a categorizar una variable continua. binner &lt;- function(x) { x &lt;- cut(x, breaks = c(0, 12, 24, 36,48,60,72), include.lowest = TRUE) as.numeric(x) } telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% step_mutate(MultipleLines = fct_collapse(MultipleLines, No = &quot;No phone service&quot;), OnlineSecurity = fct_collapse(OnlineSecurity, No = &quot;No internet service&quot;), OnlineBackup = fct_collapse(OnlineBackup, No = &quot;No internet service&quot;), DeviceProtection = fct_collapse(DeviceProtection, No = &quot;No internet service&quot;), TechSupport = fct_collapse(TechSupport, No = &quot;No internet service&quot;), StreamingTV = fct_collapse(StreamingTV, No = &quot;No internet service&quot;), StreamingMovies = fct_collapse(StreamingMovies, No = &quot;No internet service&quot;)) %&gt;% step_num2factor(tenure, transform = binner, levels = c(&quot;0-1 year&quot;, &quot;1-2 years&quot;, &quot;2-3 years&quot;, &quot;3-4 years&quot;, &quot;4-5 years&quot;, &quot;5-6 years&quot;)) %&gt;% step_rm(customerID) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() Ahora recuperamos la matriz de diseño con las funciones prep() y juice(). telco_juiced &lt;- juice(telco_rec) telco_juiced ## # A tibble: 4,930 x 28 ## SeniorCitizen MonthlyCharges TotalCharges Churn gender_Male Partner_Yes ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.442 -1.50 -0.663 No 0 0 ## 2 -0.442 0.664 0.472 No 1 1 ## 3 -0.442 -0.508 -0.546 No 0 0 ## 4 2.26 0.649 -0.850 Yes 0 0 ## 5 -0.442 0.167 -0.979 Yes 0 0 ## 6 -0.442 0.842 0.975 No 1 1 ## 7 -0.442 0.260 -0.648 No 1 0 ## 8 -0.442 -0.668 0.178 No 1 1 ## 9 -0.442 -1.20 -0.706 No 1 1 ## 10 -0.442 -0.407 -0.348 No 0 0 ## # ... with 4,920 more rows, and 22 more variables: Dependents_Yes &lt;dbl&gt;, ## # tenure_X1.2.years &lt;dbl&gt;, tenure_X2.3.years &lt;dbl&gt;, tenure_X3.4.years &lt;dbl&gt;, ## # tenure_X4.5.years &lt;dbl&gt;, tenure_X5.6.years &lt;dbl&gt;, PhoneService_Yes &lt;dbl&gt;, ## # MultipleLines_Yes &lt;dbl&gt;, InternetService_Fiber.optic &lt;dbl&gt;, ## # InternetService_No &lt;dbl&gt;, OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_Yes &lt;dbl&gt;, ## # DeviceProtection_Yes &lt;dbl&gt;, TechSupport_Yes &lt;dbl&gt;, StreamingTV_Yes &lt;dbl&gt;, ## # StreamingMovies_Yes &lt;dbl&gt;, Contract_One.year &lt;dbl&gt;, ... telco_test_bake &lt;- bake(telco_rec, new_data = telco_test) telco_test_bake ## # A tibble: 2,113 x 28 ## SeniorCitizen MonthlyCharges TotalCharges Churn gender_Male Partner_Yes ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.442 -0.271 -0.175 No 1 0 ## 2 -0.442 1.15 -0.647 Yes 0 0 ## 3 -0.442 -1.18 -0.876 No 0 0 ## 4 -0.442 -1.53 -0.865 No 1 0 ## 5 -0.442 1.38 2.25 No 1 0 ## 6 -0.442 0.830 -0.187 No 0 0 ## 7 -0.442 -1.51 -0.920 No 1 1 ## 8 -0.442 -0.183 0.302 No 1 1 ## 9 -0.442 -1.16 -0.996 Yes 1 1 ## 10 -0.442 0.837 1.80 No 1 1 ## # ... with 2,103 more rows, and 22 more variables: Dependents_Yes &lt;dbl&gt;, ## # tenure_X1.2.years &lt;dbl&gt;, tenure_X2.3.years &lt;dbl&gt;, tenure_X3.4.years &lt;dbl&gt;, ## # tenure_X4.5.years &lt;dbl&gt;, tenure_X5.6.years &lt;dbl&gt;, PhoneService_Yes &lt;dbl&gt;, ## # MultipleLines_Yes &lt;dbl&gt;, InternetService_Fiber.optic &lt;dbl&gt;, ## # InternetService_No &lt;dbl&gt;, OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_Yes &lt;dbl&gt;, ## # DeviceProtection_Yes &lt;dbl&gt;, TechSupport_Yes &lt;dbl&gt;, StreamingTV_Yes &lt;dbl&gt;, ## # StreamingMovies_Yes &lt;dbl&gt;, Contract_One.year &lt;dbl&gt;, ... Estos fueron dos ejemplos aplicados de la paquetería recipies, existen distintas funciones step que pueden implementarse en recetas para usarse con tidymodels, en las secciones siguientes les daremos su uso para ajustar un modelo completo. 6.3 Regresión lineal simple y múltiple En esta sección aprenderemos sobre regresión lineal simple y múltiple, como se ajusta un modelo de regresión en R, las métricas de desempeño para problemas de regresión y como podemos comparar modelos con estas métricas. Existen dos tipos de modelos de regresión lineal: regresión simple y regresión múltiple. La regresión lineal simple es cuando se utiliza una variable independiente para estimar una variable dependiente. Cuando se utiliza más de una variable independiente, el proceso se denomina regresión lineal múltiple. En un modelo de regresión lineal simple tratamos de explicar la relación que existe entre la variable respuesta \\(Y\\) y una única variable explicativa \\(X\\), el modelo es de la forma: \\[Y = \\beta_0 + \\beta_1X_1 + \\epsilon\\] Con \\(\\beta_0\\) la ordenada al origen, \\(\\beta_1\\) la pendiente y \\(\\epsilon_i\\) como el error aleatorio. El objetivo de un modelo de regresión múltiple es tratar de explicar la relación que existe entre una variable dependiente (variable respuesta) \\(Y\\) un conjunto de variables independientes (variables explicativas) \\(X1,..., Xm\\), el modelo es de la forma: \\[Y = \\beta_0 + \\beta_1X_1 + \\cdot \\cdot \\cdot + \\beta_mX_m + \\epsilon\\] Con: \\(Y\\) como variable respuesta. \\(X_1,X_2,...,X_m\\) como las variables explicativas, independientes o regresoras. \\(\\beta_1, \\beta_2,...,\\beta_m\\) es el efecto promedio que tiene el incremento en una unidad de la variable predictora Xi sobre la variable dependiente Y, manteniéndose constantes el resto de variables. Se conocen como coeficientes parciales de regresión. 6.3.1 Ajuste de modelo 6.3.1.1 Estimación de parámetros: Regresión lineal simple En la gran mayoría de casos, los valores \\(\\beta_0\\) y \\(\\beta_1\\) poblacionales son desconocidos, por lo que, a partir de una muestra, se obtienen sus estimaciones \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\). Estas estimaciones se conocen como coeficientes de regresión o least square coefficient estimates, ya que toman aquellos valores que minimizan la suma de cuadrados residuales, dando lugar a la recta que pasa más cerca de todos los puntos. \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\] \\[\\hat{\\beta}_1 = \\frac{\\sum^n_{i=1}(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum^n_{i=1}(x_i - \\overline{x})^2} =\\frac{S_{xy}}{S^2_x}\\] \\[\\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1\\overline{x}\\] Donde: - \\(S_{xy}\\) es la covarianza entre \\(x\\) y \\(y\\). - \\(S_{x}^{2}\\) es la varianza de \\(x\\). - \\(\\hat{\\beta}_0\\) es el valor esperado la variable \\(Y\\) cuando \\(X = 0\\), es decir, la intersección de la recta con el eje y. 6.3.1.2 Estimación de parámetros: Regresión lineal múltiple \\[Y = X\\beta + \\epsilon\\] siendo \\[Y = \\begin{pmatrix}y_1\\\\y_2\\\\.\\\\.\\\\.\\\\y_n\\end{pmatrix} \\quad \\beta = \\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\.\\\\.\\\\.\\\\\\beta_m\\end{pmatrix} \\quad \\epsilon = \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\.\\\\.\\\\.\\\\\\epsilon_n\\end{pmatrix} \\quad \\quad X = \\begin{pmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1m}\\\\1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2m}\\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{nm}\\end{pmatrix}\\\\\\] El estimador por mínimos cuadrados está dado por: \\[\\hat{\\beta} = (X^TX)^{-1}X^TY\\] 6.3.2 Residuos del modelo El residuo de una estimación se define como la diferencia entre el valor observado y el valor esperado acorde al modelo. A la hora de contemplar el conjunto de residuos hay dos posibilidades: La suma del valor absoluto de cada residuo. La suma del cuadrado de cada residuo (RSS). Esta es la aproximación más empleada (mínimos cuadrados) ya que magnifica las desviaciones más extremas. Los residuos son muy importantes puesto que en ellos se basan las diferentes métricas de desempeño del modelo. 6.3.2.1 Condiciones para el ajuste de una regresión lineal: Existen ciertas condiciones o supuestos que deben ser validados para el correcto ajuste de un modelo de regresión lineal, los cuales se enlistan a continuación: Linealidad: La relación entre ambas variables debe ser lineal. Distribución Normal de los residuos: Los residuos se tiene que distribuir de forma normal, con media igual a 0. Varianza de residuos constante (homocedasticidad): La varianza de los residuos tiene que ser aproximadamente constante. Independencia, Autocorrelación: Las observaciones deben ser independientes unas de otras. Dado que las condiciones se verifican a partir de los residuos, primero se suele generar el modelo y después se valida. 6.3.3 Métricas de desempeño Dado que nuestra variable a predecir es numérica, podemos medir qué tan cerca o lejos estuvimos del número esperado dada una predicción. Las métricas de desempeño asociadas a los problemas de regresión ocupan esa distancia cómo cuantificación del desempeño o de los errores cometidos por el modelo. Las métricas más utilizadas son: MAE: Mean Absolute Error \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}{|y_{i}-\\hat{y}_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica suma los errores absolutos de cada predicción y los divide entre el número de observaciones, para obtener el promedio absoluto del error del modelo. Una de las desventajas es que todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es muy sensible a valores atípicos, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando los errores importan lo mismo, es decir, importa lo mismo si se equivocó muy poco o se equivocó mucho. MAPE: Mean Absolute Percentage Error \\[MAPE = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{{|y_{i}-\\hat{y}_{i}|}}{|y_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es la métrica MAE expresada en porcentaje, por lo que mide el error del modelo en términos de porcentaje, al igual que con MAE, no hay errores negativos por el valor absoluto, y mientras más pequeño el error es mejor. Una de las grandes desventajas de esta métrica, es que cuando existe un valor real de 0 esta métrica no se puede calcular, por otro lado, una de las ventajas sobre MAE es que no es sensible a valores atípicos. Se recomienda utilizar esta métrica cuando en tu problema no haya valores a predecir que puedan ser 0, por ejemplo, en ventas puedes llegar a tener 0 ventas, en este caso no podemos ocupar esta métrica. Cuando los valores a predecir no tienen un rango muy grande, ya que pueden provocar errores con porcentajes sumamente altos! por ejemplo, porcentajes de un millón por ciento. En general a las personas de negocio les gusta esta métrica pues es fácil de comprender. RMSE: Root Mean Squared Error \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}{(y_{i}-\\hat{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es muy parecida a MAE, solo que en lugar de sacar el valor absoluto de la diferencia entre el valor real y el valor predicho, para evitar valores negativos eleva esta diferencia al cuadrado, y saca el promedio de esa diferencia, al final, para dejar el valor en la escala inicial saca la raíz cuadrada. Esta es la métrica más utilizada en problemas de regresión, debido a que es más fácil de optimizar que el MAE. Una de las desventajas es que todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es más sensible a valores atípicos que MAE pues eleva al cuadrado diferencias, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando en el problema que queremos resolver es muy costoso tener equivocaciones grandes, podemos tener varios errores pequeños, pero no grandes. \\(R^2\\): R cuadrada \\[R^{2} = \\frac{\\sum_{i=1}^{N}{(\\hat{y}_{i}-\\bar{y}_{i})^2}}{\\sum_{i=1}^{N}{(y_{i}-\\bar{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. \\(\\bar{y}_{i}:\\) Valor promedio de la variable y. El coeficiente de determinación es la proporción de la varianza total de la variable explicada por la regresión. El coeficiente de determinación, también llamado R cuadrado, refleja la bondad del ajuste de un modelo a la variable que pretender explicar. Es importante saber que el resultado del coeficiente de determinación oscila entre 0 y 1. Cuanto más cerca de 1 se sitúe su valor, mayor será el ajuste del modelo a la variable que estamos intentando explicar. De forma inversa, cuanto más cerca de cero, menos ajustado estará el modelo y, por tanto, menos fiable será. El problema del coeficiente de determinación, y razón por el cual surge el coeficiente de determinación ajustado, radica en que no penaliza la inclusión de variables explicativas no significativas. \\(\\bar{R}^2\\): \\(R^2\\) ajustada \\[\\bar{R}^2=1-\\frac{N-1}{N-k-1}[1-R^2]\\] Donde: \\(\\bar{R}²:\\) Es el valor de R² ajustado \\(R²:\\) Es el valor de R² original \\(N:\\) Es el total de observaciones en el ajuste \\(k:\\) Es el número de variables usadas en el modelo El coeficiente de determinación ajustado (R cuadrado ajustado) es la medida que define el porcentaje explicado por la varianza de la regresión en relación con la varianza de la variable explicada. Es decir, lo mismo que el R cuadrado, pero con una diferencia: El coeficiente de determinación ajustado penaliza la inclusión de variables. En la fórmula, N es el tamaño de la muestra y k el número de variables explicativas. RMSLE: Root Mean Squared Logarithmic Loss Error \\[e = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (log(1+p(i))-log(1+o(i)))^2}\\] Donde: \\(p(i)\\) son las predicciones y \\(o(i)\\) son las observaciones, esto implica que nos interesa más evaluar error relativo que absoluto, pues: \\[log(1+p)-log(1+o)=log(\\frac{1+p}{1+o})\\approx log(\\frac{p}{o})=log(1+\\frac{p-o}{o})\\approx\\frac{p-o}{p}\\] cuando el error relativo \\(\\frac{|p-o|}{|o|}\\) es chico, menor a .2 por ejemplo, y \\(o\\) y \\(p\\) son grandes. 6.3.4 Implementación en R Ajustaremos un modelo de regresión usando la receta antes vista. modelo1 &lt;- linear_reg() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;lm&quot;) lm_fit1 &lt;- fit(modelo1, Log_SalePrice ~ ., casa_juiced) p_test &lt;- predict(lm_fit1,casa_test_bake) %&gt;% bind_cols(ames_test) %&gt;% dplyr::mutate(.pred=exp(.pred), Log_SalePrice = exp(Log_SalePrice)) 6.3.5 Coeficientes del modelo Podemos recuperar los coeficientes de nuestro modelo con la función tidy() y observar cuales variables explicativas son las más significativas de acuerdo con el p-value. tidy(lm_fit1) %&gt;% as.data.frame() %&gt;% arrange(p.value) ## term estimate ## 1 (Intercept) 6.5354314099 ## 2 Roof_Matl_WdShngl 2.9893794380 ## 3 Roof_Matl_CompShg 2.8257974384 ## 4 Roof_Matl_Tar.Grv 2.8787190565 ## 5 Roof_Matl_WdShake 2.7974974577 ## 6 Roof_Matl_Metal 2.9792835932 ## 7 Neighborhood_Edwards_x_Condition_1_Pos -1.5748565752 ## 8 Misc_Feature_Gar2 2.1670064098 ## 9 Misc_Feature_Othr 2.2122118312 ## 10 Misc_Feature_Shed 2.1709044247 ## 11 Misc_Feature_None 2.1887088934 ## 12 Sale_Condition_Normal 0.0839961046 ## 13 Year_Built 0.0674479691 ## 14 Neighborhood_Crawfor_Greens 0.1526884954 ## 15 AvgRoomSF 0.0686032999 ## 16 Lot_Area 0.0208233217 ## 17 Misc_Feature_TenC 1.9892020456 ## 18 TotRms_AbvGrd 0.1151681993 ## 19 Bsmt_Unf_SF -0.0309655996 ## 20 Functional_Typ 0.1131962662 ## 21 TotalSF 0.1060809777 ## 22 Exter_Cond_Fair -0.0997796219 ## 23 Sale_Condition_AdjLand 0.2643070490 ## 24 Overall_Cond_Excellent 0.3524857381 ## 25 Age_House -0.0183384217 ## 26 Neighborhood_Veenker_StoneBr 0.0970178713 ## 27 Fireplaces 0.0133104968 ## 28 Screen_Porch 0.0107270820 ## 29 MS_Zoning_C_all -0.1706163288 ## 30 Exterior_1st_BrkFace 0.2239717294 ## 31 Garage_Cars 0.0252536426 ## 32 Overall_Cond_Very_Good 0.2926290854 ## 33 Neighborhood_Northridge_Heights 0.0882592651 ## 34 Neighborhood_Brookside 0.0779963375 ## 35 Overall_Cond_Good 0.2860238412 ## 36 BsmtFin_Type_1_Unf_x_Total_Bsmt_SF 0.0397949975 ## 37 BsmtFin_Type_1_GLQ 0.0276098602 ## 38 BsmtFin_Type_1_GLQ_x_Total_Bsmt_SF 0.0345315499 ## 39 Bsmt_Exposure_Gd 0.0353197356 ## 40 Functional_Sal -1.0761183495 ## 41 Overall_Cond_Above_Average 0.2404090257 ## 42 Heating_QC_Typical_x_TotRms_AbvGrd_x_TotalSF -0.0220879103 ## 43 Enclosed_Porch 0.0095639082 ## 44 Mas_Vnr_Type_Stone 0.0804125434 ## 45 Exterior_1st_MetalSd 0.1828853306 ## 46 MS_Zoning_I_R_M_H -0.0751899138 ## 47 Heating_QC_Fair_x_TotRms_AbvGrd_x_TotalSF -0.0544034768 ## 48 Bsmt_Exposure_No -0.0228970399 ## 49 Overall_Cond_Average 0.2087603408 ## 50 Exterior_1st_Plywood 0.1441245562 ## 51 Neighborhood_Crawfor_Greens_x_Condition_1_Artery_Feedr -0.1476469387 ## 52 Half_Bath 0.0117809480 ## 53 MS_SubClass_One_Story_1945_and_Older -0.0492080738 ## 54 Exterior_1st_Stucco 0.1554368145 ## 55 BsmtFin_Type_1_Rec_BLQ -0.0221142908 ## 56 BsmtFin_Type_1_LwQ -0.0338870742 ## 57 Exterior_1st_Wd.Sdng 0.1347208363 ## 58 Bsmt_Cond_No_Basement_x_TotRms_AbvGrd -0.0501573873 ## 59 Neighborhood_Brookside_x_Condition_1_Artery_Feedr -0.1116772608 ## 60 Heating_QC_Typical -0.0220156731 ## 61 Functional_Min 0.0541781372 ## 62 MS_SubClass_Two_Story_PUD_1946_and_Newer -0.1448295406 ## 63 Exterior_1st_HdBoard 0.1292773150 ## 64 MS_SubClass_Duplex_All_Styles_and_Ages -0.1374460679 ## 65 Alley_No_Alley_Access -0.0354048327 ## 66 Condition_1_Artery_Feedr -0.0434995649 ## 67 Garage_Type_CarPort -0.0766526597 ## 68 Exterior_1st_WdShing 0.1309557911 ## 69 Neighborhood_Meadow_Village -0.1014042307 ## 70 Heating_QC_Typical_x_TotRms_AbvGrd_x_TotalSF_x_Second_Flr_SF 0.0119766974 ## 71 Mas_Vnr_Type_None 0.0572560499 ## 72 BsmtFin_Type_2_GLQ 0.0695531289 ## 73 Foundation_PConc 0.0269253504 ## 74 Mas_Vnr_Type_CBlock -0.2874464592 ## 75 Mas_Vnr_Type_BrkFace 0.0541852949 ## 76 Heating_QC_Good_x_TotRms_AbvGrd_x_TotalSF_x_Second_Flr_SF 0.0139938625 ## 77 Heating_QC_Good_x_TotRms_AbvGrd -0.0276816203 ## 78 Heating_QC_Poor_x_TotRms_AbvGrd 0.2902387066 ## 79 Mas_Vnr_Area 0.0085752190 ## 80 Neighborhood_SWISU_Sawyer_x_Condition_1_Pos 0.1776648648 ## 81 Sale_Type_New 0.1004120920 ## 82 Age_House_x_TotRms_AbvGrd -0.0073082891 ## 83 Exterior_1st_VinylSd 0.1215843583 ## 84 BsmtFin_Type_1_Rec_BLQ_x_BsmtFin_SF_1 -0.0152693653 ## 85 Bedroom_AbvGr -0.0237634823 ## 86 Foundation_Slab 0.0692662261 ## 87 Neighborhood_Edwards -0.0427772860 ## 88 TotalSF_x_TotRms_AbvGrd -0.0077473875 ## 89 Electrical_Mix 0.3294516510 ## 90 Neighborhood_Northwest_Ames -0.0344854292 ## 91 BsmtFin_SF_2 -0.0089771237 ## 92 Neighborhood_Veenker_StoneBr_x_Condition_1_Artery_Feedr -0.1205737566 ## 93 BsmtFin_Type_2_No_Basement -0.2114694625 ## 94 Sale_Type_Con 0.0923800805 ## 95 Land_Slope_Mod_Sev -0.0278039704 ## 96 Lot_Frontage 0.0053771820 ## 97 BsmtFin_Type_1_Rec_BLQ_x_BsmtFin_SF_1_x_Total_Bsmt_SF 0.0190756972 ## 98 Garage_Cond_Fair -0.1390094571 ## 99 Overall_Cond_Below_Average 0.1379207139 ## 100 Neighborhood_NoRidge_GrnHill 0.0441954786 ## 101 Bsmt_Cond_Fair -0.0330267843 ## 102 Neighborhood_ClearCr_Somerst 0.0426228644 ## 103 Functional_Sev -0.1649433779 ## 104 Exterior_2nd_BrkFace -0.1000199837 ## 105 Garage_Finish_Unf -0.0148582686 ## 106 Bsmt_Cond_Typical_x_TotRms_AbvGrd -0.0196953472 ## 107 Exter_Cond_Poor 0.1590587668 ## 108 Heating_QC_Typical_x_TotalSF -0.0203067291 ## 109 House_Style_SFoyer 0.0695697426 ## 110 Heating_QC_Good_x_Second_Flr_SF 0.0170224447 ## 111 Garage_Type_More_Than_Two_Types -0.0454619258 ## 112 Neighborhood_Brookside_x_Condition_1_Pos 0.2000128576 ## 113 Lot_Config_Inside -0.0109087587 ## 114 Exterior_1st_CemntBd 0.1067339761 ## 115 Sale_Condition_Family 0.0369504708 ## 116 MS_SubClass_Two_Story_1945_and_Older 0.0502667441 ## 117 Heating_QC_Fair_x_TotalSF_x_Second_Flr_SF 0.0572836205 ## 118 Year_Sold -0.0039496403 ## 119 Paved_Drive_Paved 0.0194573348 ## 120 Condition_2_RRAe -0.2377861235 ## 121 Heating_QC_Good_x_TotalSF_x_Second_Flr_SF -0.0174060124 ## 122 BsmtFin_Type_2_Rec_BLQ -0.0309250685 ## 123 BsmtFin_Type_2_Unf -0.0345684777 ## 124 Lot_Config_FR2 -0.0216972451 ## 125 Neighborhood_Gilbert -0.0301703670 ## 126 Garage_Area 0.0089591587 ## 127 Exterior_2nd_MetalSd -0.0878145596 ## 128 Roof_Style_Mansard 0.1199002214 ## 129 TotalBaths 0.0082344726 ## 130 Bsmt_Exposure_Mn -0.0159901293 ## 131 Neighborhood_ClearCr_Somerst_x_Condition_1_Pos 0.1127874885 ## 132 MS_Zoning_Residential_Low_Density -0.0320703088 ## 133 Heating_QC_Good_x_TotRms_AbvGrd_x_TotalSF -0.0158575718 ## 134 Heating_QC_Fair_x_TotRms_AbvGrd -0.0353555502 ## 135 Pool_QC_Good 0.1778294439 ## 136 MS_SubClass_PUD_Multilevel_Split_Level_Foyer -0.0992067685 ## 137 Lot_Shape_IRREG 0.0085239453 ## 138 Exterior_1st_BrkComm 0.0933653854 ## 139 ThirdSsn_Porch 0.0032936117 ## 140 Bedroom_AbvGr_o_Gr_Liv_Area 0.0150892447 ## 141 Garage_Finish_RFn -0.0092874350 ## 142 Full_Bath 0.0070893648 ## 143 Roof_Style_Hip 0.0893280338 ## 144 Neighborhood_Northwest_Ames_x_Condition_1_Pos 0.0622450497 ## 145 MS_SubClass_One_Story_PUD_1946_and_Newer -0.0632352558 ## 146 House_Style_One_and_Half_Unf 0.0793206380 ## 147 Overall_Cond_Poor -0.1096508642 ## 148 Lot_Config_FR3 0.0479159235 ## 149 MS_SubClass_Split_or_Multilevel -0.0768770326 ## 150 House_Style_SLvl 0.0761410086 ## 151 Central_Air_Y 0.0162089259 ## 152 Neighborhood_BrDale_IDOTRR_x_Condition_1_Artery_Feedr -0.0586735384 ## 153 Latitude 0.0096764508 ## 154 Heating_QC_Fair_x_TotalSF 0.0358349625 ## 155 Exterior_1st_AsphShn 0.1424962737 ## 156 Exterior_2nd_Plywood -0.0598727511 ## 157 Garage_Type_No_Garage -0.1024785400 ## 158 MS_SubClass_Split_Foyer -0.0438433716 ## 159 Neighborhood_Veenker_StoneBr_x_Condition_1_Pos 0.1318256502 ## 160 Roof_Style_Gable 0.0758356350 ## 161 Foundation_Wood_Stone 0.0364434243 ## 162 Heating_QC_Poor -0.0962322587 ## 163 BsmtFin_Type_1_LwQ_x_Total_Bsmt_SF 0.0199569246 ## 164 Utilities_NoSeWa -0.1322510742 ## 165 Exterior_2nd_CBlock -0.1519152624 ## 166 BsmtFin_SF_1 0.0042706231 ## 167 Heating_Grav_Wall -0.1362700500 ## 168 Overall_Cond_Fair 0.0818091162 ## 169 Longitude -0.0095343967 ## 170 Neighborhood_Sawyer_West_x_Condition_1_Artery_Feedr 0.0479595096 ## 171 Neighborhood_Old_Town -0.0197120466 ## 172 Condition_1_Pos -0.0396604248 ## 173 Condition_2_PosN -0.1071842000 ## 174 Open_Porch_SF 0.0032894354 ## 175 Sale_Type_WD. 0.0150797698 ## 176 Garage_Cond_Good -0.0828918268 ## 177 Neighborhood_Blueste_Mitchel 0.0283399874 ## 178 Foundation_CBlock 0.0111350120 ## 179 Exterior_2nd_HdBoard -0.0509721544 ## 180 Exterior_2nd_Other -0.1140946941 ## 181 Land_Contour_Low_HLS 0.0124025872 ## 182 Wood_Deck_SF 0.0025286230 ## 183 Neighborhood_BrDale_IDOTRR 0.0197768894 ## 184 Neighborhood_ClearCr_Somerst_x_Condition_1_Artery_Feedr 0.0379667905 ## 185 Heating_QC_Fair_x_Second_Flr_SF 0.0228038622 ## 186 Garage_Cond_Poor -0.0736268948 ## 187 Second_Flr_SF_x_First_Flr_SF 0.0038984336 ## 188 Neighborhood_Brookside_x_Condition_1_Railr -0.1030327715 ## 189 Neighborhood_Timberland 0.0242744253 ## 190 Fence_Good_Wood -0.0146990608 ## 191 Neighborhood_Sawyer_West -0.0231185442 ## 192 Bsmt_Cond_Typical -0.0096976115 ## 193 Electrical_SBrkr -0.0094566273 ## 194 Bldg_Type_TwnhsE 0.0395424470 ## 195 Misc_Val 0.0044256735 ## 196 Bsmt_Cond_Poor -0.0687738514 ## 197 Neighborhood_Blueste_Mitchel_x_Condition_1_Artery_Feedr 0.0577759423 ## 198 Neighborhood_Edwards_x_Condition_1_Artery_Feedr -0.0296850431 ## 199 Garage_Cond_Typical -0.0534896450 ## 200 Heating_QC_Good_x_TotRms_AbvGrd_x_Second_Flr_SF -0.0077537952 ## 201 Garage_Type_BuiltIn -0.0089065709 ## 202 Bsmt_Cond_Poor_x_TotRms_AbvGrd 0.0853601775 ## 203 Sale_Condition_Partial 0.0319361478 ## 204 Neighborhood_Northwest_Ames_x_Condition_1_Artery_Feedr 0.0333242020 ## 205 Fence_Minimum_Privacy 0.0097206868 ## 206 Heating_QC_Fair_x_TotRms_AbvGrd_x_TotalSF_x_Second_Flr_SF 0.0115507886 ## 207 Heating_QC_Fair 0.0146446254 ## 208 BsmtFin_Type_1_Rec_BLQ_x_Total_Bsmt_SF -0.0083374937 ## 209 House_Style_Two_and_Half_Fin -0.0468203921 ## 210 MS_Zoning_A_agr -0.1407750641 ## 211 Heating_QC_Good -0.0058216481 ## 212 Neighborhood_SWISU_Sawyer_x_Condition_1_Railr -0.0750481399 ## 213 Neighborhood_SWISU_Sawyer_x_Condition_1_Artery_Feedr -0.0174590437 ## 214 Condition_2_Feedr -0.0439323459 ## 215 Heating_QC_Typical_x_TotalSF_x_Second_Flr_SF -0.0071249443 ## 216 Neighborhood_BrDale_IDOTRR_x_Condition_1_Railr 0.0826897804 ## 217 House_Style_Two_and_Half_Unf -0.0281340623 ## 218 Street_Pave 0.0280257225 ## 219 Pool_QC_Fair 0.0754068598 ## 220 Roof_Style_Shed 0.0562364879 ## 221 Neighborhood_SWISU_Sawyer 0.0120311815 ## 222 Bsmt_Cond_No_Basement 0.0877416916 ## 223 Sale_Type_ConLD 0.0167335013 ## 224 Bsmt_Exposure_No_Basement -0.0568981562 ## 225 Heating_QC_Good_x_TotalSF -0.0063438110 ## 226 Exterior_2nd_Wd.Sdng -0.0276239437 ## 227 Alley_Paved -0.0112216665 ## 228 Heating_QC_Fair_x_TotRms_AbvGrd_x_Second_Flr_SF -0.0122907647 ## 229 Bldg_Type_TwoFmCon -0.0352164717 ## 230 Exterior_2nd_AsphShn -0.0453983105 ## 231 Neighborhood_Northridge_Heights_x_Condition_1_Pos 0.0325817978 ## 232 Exterior_2nd_CmentBd 0.0309503233 ## 233 Mo_Sold 0.0010223622 ## 234 Exterior_1st_CBlock 0.0726306514 ## 235 Neighborhood_Sawyer_West_x_Condition_1_Railr -0.0496085902 ## 236 Second_Flr_SF_o_First_Flr_SF -0.0037535081 ## 237 Neighborhood_Blmngtn_CollgCr -0.0101898927 ## 238 Sale_Condition_Alloca 0.0152193541 ## 239 Condition_2_PosA 0.0519901942 ## 240 House_Style_One_Story 0.0122776990 ## 241 Porch_TRUE. 0.0027611020 ## 242 MS_SubClass_One_and_Half_Story_Finished_All_Ages -0.0120894325 ## 243 MS_SubClass_Two_and_Half_Story_All_Ages 0.0190554127 ## 244 Exterior_2nd_VinylSd -0.0217600334 ## 245 Heating_QC_Typical_x_TotRms_AbvGrd 0.0039506701 ## 246 Heating_QC_Typical_x_TotRms_AbvGrd_x_Second_Flr_SF -0.0027876882 ## 247 Condition_2_Norm -0.0215290382 ## 248 Paved_Drive_Partial_Pavement 0.0071331948 ## 249 Sale_Type_ConLw 0.0221054985 ## 250 MS_SubClass_One_Story_with_Finished_Attic_All_Ages 0.0177211423 ## 251 Neighborhood_Landmark 0.0343858777 ## 252 Neighborhood_ClearCr_Somerst_x_Condition_1_Railr 0.0381982137 ## 253 Pool_QC_Typical -0.0308945130 ## 254 Electrical_FuseF -0.0071178965 ## 255 House_Style_Two_Story -0.0083925786 ## 256 MS_SubClass_Two_Story_1946_and_Newer -0.0087184067 ## 257 Utilities_NoSewr 0.0286387480 ## 258 MS_SubClass_One_and_Half_Story_Unfinished_All_Ages -0.0179367426 ## 259 Garage_Type_Basment 0.0063404609 ## 260 Exterior_2nd_Stucco 0.0150551123 ## 261 Sale_Type_ConLI 0.0115762056 ## 262 Sale_Type_CWD -0.0102686970 ## 263 Neighborhood_Old_Town_x_Condition_1_Railr 0.0357914967 ## 264 Lot_Config_CulDSac -0.0029739279 ## 265 Exterior_2nd_Wd.Shng -0.0129046923 ## 266 Neighborhood_Blmngtn_CollgCr_x_Condition_1_Pos 0.0272725608 ## 267 Roof_Style_Gambrel 0.0151178667 ## 268 Condition_1_Railr -0.0228620737 ## 269 Heating_QC_Typical_x_Second_Flr_SF 0.0017394752 ## 270 Exterior_2nd_Brk.Cmn -0.0103449307 ## 271 Neighborhood_Old_Town_x_Condition_1_Artery_Feedr 0.0043958326 ## 272 MS_SubClass_Two_Family_conversion_All_Styles_and_Ages -0.0116423063 ## 273 Neighborhood_Northwest_Ames_x_Condition_1_Railr 0.0188487369 ## 274 Neighborhood_Blmngtn_CollgCr_x_Condition_1_Artery_Feedr 0.0141249623 ## 275 Neighborhood_Crawfor_Greens_x_Condition_1_Pos 0.0139722267 ## 276 Heating_GasA_W -0.0125071196 ## 277 Garage_Type_Detchd -0.0007823754 ## 278 Electrical_FuseP 0.0042189766 ## 279 Fence_Minimum_Wood_Wire 0.0025405048 ## 280 Pool -0.0003028168 ## 281 Neighborhood_Gilbert_x_Condition_1_Artery_Feedr 0.0051362083 ## 282 Bsmt_Cond_Fair_x_TotRms_AbvGrd 0.0009518697 ## 283 Exterior_2nd_ImStucc -0.0027362718 ## 284 Second_Flr_SF_x_Bedroom_AbvGr 0.0000592300 ## 285 Garage_Finish_No_Garage -0.0014380255 ## 286 Fence_No_Fence -0.0001538172 ## 287 Bldg_Type_Du_Tu -0.0005311585 ## 288 Exterior_2nd_Stone 0.0002901444 ## 289 Neighborhood_Gilbert_x_Condition_1_Railr -0.0002001430 ## 290 MS_SubClass_One_and_Half_Story_PUD_All_Ages NA ## 291 Alley_unknown NA ## 292 Neighborhood_Hayden_Lake NA ## 293 Condition_2_RRAn NA ## 294 Condition_2_RRNn NA ## 295 Overall_Cond_Very_Excellent NA ## 296 Roof_Matl_Membran NA ## 297 Roof_Matl_Roll NA ## 298 Exterior_1st_ImStucc NA ## 299 Exterior_1st_PreCast NA ## 300 Exterior_1st_Stone NA ## 301 Exterior_2nd_PreCast NA ## 302 Mas_Vnr_Type_unknown NA ## 303 Bsmt_Cond_unknown NA ## 304 Bsmt_Exposure_unknown NA ## 305 BsmtFin_Type_1_No_Basement NA ## 306 BsmtFin_Type_1_unknown NA ## 307 BsmtFin_Type_2_unknown NA ## 308 Heating_QC_unknown NA ## 309 Electrical_Unknown NA ## 310 Electrical_unknown NA ## 311 Garage_Type_unknown NA ## 312 Garage_Finish_unknown NA ## 313 Garage_Cond_No_Garage NA ## 314 Garage_Cond_unknown NA ## 315 Pool_QC_No_Pool NA ## 316 Pool_QC_unknown NA ## 317 Fence_unknown NA ## 318 Bsmt_Cond_unknown_x_TotRms_AbvGrd NA ## 319 BsmtFin_Type_1_GLQ_x_BsmtFin_SF_1 NA ## 320 BsmtFin_Type_1_LwQ_x_BsmtFin_SF_1 NA ## 321 BsmtFin_Type_1_No_Basement_x_BsmtFin_SF_1 NA ## 322 BsmtFin_Type_1_Unf_x_BsmtFin_SF_1 NA ## 323 BsmtFin_Type_1_unknown_x_BsmtFin_SF_1 NA ## 324 BsmtFin_Type_1_No_Basement_x_Total_Bsmt_SF NA ## 325 BsmtFin_Type_1_unknown_x_Total_Bsmt_SF NA ## 326 BsmtFin_Type_1_GLQ_x_BsmtFin_SF_1_x_Total_Bsmt_SF NA ## 327 BsmtFin_Type_1_LwQ_x_BsmtFin_SF_1_x_Total_Bsmt_SF NA ## 328 BsmtFin_Type_1_No_Basement_x_BsmtFin_SF_1_x_Total_Bsmt_SF NA ## 329 BsmtFin_Type_1_Unf_x_BsmtFin_SF_1_x_Total_Bsmt_SF NA ## 330 BsmtFin_Type_1_unknown_x_BsmtFin_SF_1_x_Total_Bsmt_SF NA ## 331 Heating_QC_unknown_x_TotRms_AbvGrd NA ## 332 Heating_QC_Poor_x_TotalSF NA ## 333 Heating_QC_unknown_x_TotalSF NA ## 334 Heating_QC_Poor_x_TotRms_AbvGrd_x_TotalSF NA ## 335 Heating_QC_unknown_x_TotRms_AbvGrd_x_TotalSF NA ## 336 Heating_QC_Poor_x_Second_Flr_SF NA ## 337 Heating_QC_unknown_x_Second_Flr_SF NA ## 338 Heating_QC_Poor_x_TotRms_AbvGrd_x_Second_Flr_SF NA ## 339 Heating_QC_unknown_x_TotRms_AbvGrd_x_Second_Flr_SF NA ## 340 Heating_QC_Poor_x_TotalSF_x_Second_Flr_SF NA ## 341 Heating_QC_unknown_x_TotalSF_x_Second_Flr_SF NA ## 342 Heating_QC_Poor_x_TotRms_AbvGrd_x_TotalSF_x_Second_Flr_SF NA ## 343 Heating_QC_unknown_x_TotRms_AbvGrd_x_TotalSF_x_Second_Flr_SF NA ## 344 Neighborhood_Blmngtn_CollgCr_x_Condition_1_Railr NA ## 345 Neighborhood_Old_Town_x_Condition_1_Pos NA ## 346 Neighborhood_Edwards_x_Condition_1_Railr NA ## 347 Neighborhood_Northridge_Heights_x_Condition_1_Artery_Feedr NA ## 348 Neighborhood_Northridge_Heights_x_Condition_1_Railr NA ## 349 Neighborhood_Gilbert_x_Condition_1_Pos NA ## 350 Neighborhood_Sawyer_West_x_Condition_1_Pos NA ## 351 Neighborhood_Blueste_Mitchel_x_Condition_1_Pos NA ## 352 Neighborhood_Blueste_Mitchel_x_Condition_1_Railr NA ## 353 Neighborhood_Crawfor_Greens_x_Condition_1_Railr NA ## 354 Neighborhood_BrDale_IDOTRR_x_Condition_1_Pos NA ## 355 Neighborhood_Timberland_x_Condition_1_Artery_Feedr NA ## 356 Neighborhood_Timberland_x_Condition_1_Pos NA ## 357 Neighborhood_Timberland_x_Condition_1_Railr NA ## 358 Neighborhood_NoRidge_GrnHill_x_Condition_1_Artery_Feedr NA ## 359 Neighborhood_NoRidge_GrnHill_x_Condition_1_Pos NA ## 360 Neighborhood_NoRidge_GrnHill_x_Condition_1_Railr NA ## 361 Neighborhood_Veenker_StoneBr_x_Condition_1_Railr NA ## 362 Neighborhood_Meadow_Village_x_Condition_1_Artery_Feedr NA ## 363 Neighborhood_Meadow_Village_x_Condition_1_Pos NA ## 364 Neighborhood_Meadow_Village_x_Condition_1_Railr NA ## 365 Neighborhood_Landmark_x_Condition_1_Artery_Feedr NA ## 366 Neighborhood_Landmark_x_Condition_1_Pos NA ## 367 Neighborhood_Landmark_x_Condition_1_Railr NA ## 368 Neighborhood_Hayden_Lake_x_Condition_1_Artery_Feedr NA ## 369 Neighborhood_Hayden_Lake_x_Condition_1_Pos NA ## 370 Neighborhood_Hayden_Lake_x_Condition_1_Railr NA ## std.error statistic p.value ## 1 0.456659562 14.311386315 4.936044e-44 ## 2 0.214424181 13.941428762 5.523929e-42 ## 3 0.211136461 13.383749178 5.624083e-39 ## 4 0.219437346 13.118637755 1.397254e-37 ## 5 0.217412475 12.867235224 2.801625e-36 ## 6 0.248637577 11.982434946 7.326143e-32 ## 7 0.163857520 -9.611133971 2.371371e-21 ## 8 0.244105934 8.877319687 1.647118e-18 ## 9 0.260794395 8.482589623 4.593329e-17 ## 10 0.267581923 8.113045911 9.158783e-16 ## 11 0.271933451 8.048693111 1.523519e-15 ## 12 0.011334322 7.410774428 1.938906e-13 ## 13 0.009497976 7.101299049 1.785786e-12 ## 14 0.021842686 6.990371747 3.875793e-12 ## 15 0.010666181 6.431852327 1.619536e-10 ## 16 0.003262358 6.382904210 2.216124e-10 ## 17 0.313919968 6.336653441 2.974546e-10 ## 18 0.018382315 6.265163102 4.670216e-10 ## 19 0.005264986 -5.881421522 4.856095e-09 ## 20 0.019878930 5.694283685 1.448608e-08 ## 21 0.018902395 5.612038843 2.318128e-08 ## 22 0.020019867 -4.984030327 6.838556e-07 ## 23 0.056383162 4.687694715 2.975340e-06 ## 24 0.081994093 4.298916252 1.810210e-05 ## 25 0.004491041 -4.083334440 4.638231e-05 ## 26 0.023789102 4.078248595 4.739871e-05 ## 27 0.003276675 4.062196312 5.074717e-05 ## 28 0.002641985 4.060236291 5.117107e-05 ## 29 0.042831833 -3.983400137 7.070165e-05 ## 30 0.056444448 3.968002836 7.538413e-05 ## 31 0.006712071 3.762422011 1.737811e-04 ## 32 0.079744885 3.669565602 2.501911e-04 ## 33 0.024170539 3.651522389 2.683022e-04 ## 34 0.021540167 3.620971796 3.017998e-04 ## 35 0.079165119 3.613003343 3.111604e-04 ## 36 0.011370522 3.499839076 4.771187e-04 ## 37 0.008544537 3.231288212 1.255075e-03 ## 38 0.010961300 3.150315081 1.658336e-03 ## 39 0.011282648 3.130447337 1.774039e-03 ## 40 0.349088200 -3.082654615 2.083433e-03 ## 41 0.079027945 3.042076141 2.384210e-03 ## 42 0.007269290 -3.038523815 2.412350e-03 ## 43 0.003169867 3.017132527 2.588331e-03 ## 44 0.027377481 2.937178267 3.355131e-03 ## 45 0.062419056 2.929959885 3.433670e-03 ## 46 0.026481645 -2.839321894 4.572824e-03 ## 47 0.019892439 -2.734882223 6.302779e-03 ## 48 0.008629747 -2.653268970 8.043257e-03 ## 49 0.078737243 2.651354474 8.088812e-03 ## 50 0.054385396 2.650059900 8.119746e-03 ## 51 0.055759249 -2.647936301 8.170720e-03 ## 52 0.004468213 2.636612882 8.447394e-03 ## 53 0.018944039 -2.597549241 9.467362e-03 ## 54 0.060274585 2.578811843 9.994541e-03 ## 55 0.008591296 -2.574034218 1.013308e-02 ## 56 0.013401563 -2.528591280 1.153880e-02 ## 57 0.053717887 2.507932528 1.223326e-02 ## 58 0.020133615 -2.491226110 1.282170e-02 ## 59 0.046193466 -2.417598646 1.572421e-02 ## 60 0.009172537 -2.400172792 1.649052e-02 ## 61 0.022878815 2.368048175 1.798962e-02 ## 62 0.061420147 -2.358013580 1.848176e-02 ## 63 0.054975667 2.351537010 1.880562e-02 ## 64 0.058869177 -2.334771351 1.966717e-02 ## 65 0.015322102 -2.310703352 2.096422e-02 ## 66 0.018883395 -2.303588150 2.136169e-02 ## 67 0.033551418 -2.284632480 2.245279e-02 ## 68 0.057437104 2.279985986 2.272754e-02 ## 69 0.044484996 -2.279515331 2.275553e-02 ## 70 0.005422994 2.208502714 2.733757e-02 ## 71 0.026074777 2.195840445 2.823331e-02 ## 72 0.032207818 2.159510723 3.094511e-02 ## 73 0.012755203 2.110930839 3.491880e-02 ## 74 0.137646754 -2.088290869 3.691486e-02 ## 75 0.025962299 2.087076117 3.702465e-02 ## 76 0.006770497 2.066888434 3.889044e-02 ## 77 0.013419837 -2.062739012 3.928368e-02 ## 78 0.141250239 2.054783830 4.004706e-02 ## 79 0.004193580 2.044844464 4.101850e-02 ## 80 0.089303439 1.989451550 4.680576e-02 ## 81 0.050609832 1.984043173 4.740597e-02 ## 82 0.003708078 -1.970910157 4.889045e-02 ## 83 0.062068048 1.958888067 5.028341e-02 ## 84 0.007812493 -1.954480443 5.080238e-02 ## 85 0.012182096 -1.950689096 5.125238e-02 ## 86 0.035585674 1.946463811 5.175780e-02 ## 87 0.022167688 -1.929713499 5.380272e-02 ## 88 0.004026025 -1.924326599 5.447454e-02 ## 89 0.171465144 1.921391384 5.484354e-02 ## 90 0.018176867 -1.897215300 5.796289e-02 ## 91 0.004752288 -1.889010689 5.905448e-02 ## 92 0.063856200 -1.888207504 5.916225e-02 ## 93 0.112742127 -1.875691619 6.086287e-02 ## 94 0.051763554 1.784654892 7.448931e-02 ## 95 0.015590927 -1.783342944 7.470258e-02 ## 96 0.003033019 1.772880874 7.642118e-02 ## 97 0.010848110 1.758435090 7.884709e-02 ## 98 0.079840307 -1.741093712 8.184169e-02 ## 99 0.079450260 1.735937846 8.274963e-02 ## 100 0.025944465 1.703464647 8.865749e-02 ## 101 0.019803923 -1.667688993 9.555518e-02 ## 102 0.025572794 1.666726932 9.574644e-02 ## 103 0.099119635 -1.664083785 9.627350e-02 ## 104 0.060402566 -1.655889653 9.792223e-02 ## 105 0.009004923 -1.650016098 9.911788e-02 ## 106 0.011949483 -1.648217461 9.948634e-02 ## 107 0.096859038 1.642167533 1.007337e-01 ## 108 0.012453497 -1.630604608 1.031525e-01 ## 109 0.043028889 1.616814748 1.060973e-01 ## 110 0.010598465 1.606123649 1.084259e-01 ## 111 0.028721751 -1.582839656 1.136375e-01 ## 112 0.126726987 1.578297273 1.146768e-01 ## 113 0.007008679 -1.556464317 1.197773e-01 ## 114 0.069302431 1.540118794 1.237110e-01 ## 115 0.024117732 1.532087291 1.256804e-01 ## 116 0.033017631 1.522421272 1.280830e-01 ## 117 0.037640878 1.521846002 1.282271e-01 ## 118 0.002615959 -1.509825173 1.312673e-01 ## 119 0.013148665 1.479795513 1.391066e-01 ## 120 0.164280181 -1.447442549 1.479509e-01 ## 121 0.012276276 -1.417857650 1.564091e-01 ## 122 0.021848777 -1.415414164 1.571238e-01 ## 123 0.024423170 -1.415396813 1.571289e-01 ## 124 0.015379969 -1.410746978 1.584958e-01 ## 125 0.021467341 -1.405407740 1.600764e-01 ## 126 0.006387204 1.402673101 1.608905e-01 ## 127 0.063066144 -1.392419993 1.639709e-01 ## 128 0.086505794 1.386036895 1.659110e-01 ## 129 0.005960734 1.381452886 1.673149e-01 ## 130 0.011621986 -1.375851748 1.690424e-01 ## 131 0.082403475 1.368722486 1.712605e-01 ## 132 0.023570163 -1.360631605 1.738042e-01 ## 133 0.011772937 -1.346951192 1.781692e-01 ## 134 0.026362925 -1.341108771 1.800580e-01 ## 135 0.132900346 1.338066071 1.810476e-01 ## 136 0.074235601 -1.336377244 1.815985e-01 ## 137 0.006428309 1.326001268 1.850111e-01 ## 138 0.073194681 1.275576088 2.022736e-01 ## 139 0.002584522 1.274359975 2.027039e-01 ## 140 0.012105942 1.246432967 2.127712e-01 ## 141 0.007473032 -1.242793372 2.141093e-01 ## 142 0.005753049 1.232279488 2.180091e-01 ## 143 0.072570743 1.230909728 2.185209e-01 ## 144 0.051167089 1.216505587 2.239553e-01 ## 145 0.052019047 -1.215617335 2.242936e-01 ## 146 0.065415675 1.212563169 2.254594e-01 ## 147 0.091907146 -1.193061362 2.330060e-01 ## 148 0.040408722 1.185781704 2.358684e-01 ## 149 0.064960597 -1.183440977 2.367941e-01 ## 150 0.065056075 1.170390451 2.420021e-01 ## 151 0.013857285 1.169704323 2.422782e-01 ## 152 0.050224219 -1.168231964 2.428713e-01 ## 153 0.008360225 1.157439085 2.472499e-01 ## 154 0.031731306 1.129325162 2.589144e-01 ## 155 0.128019538 1.113082240 2.658249e-01 ## 156 0.054212238 -1.104413942 2.695644e-01 ## 157 0.093489497 -1.096150294 2.731628e-01 ## 158 0.040243445 -1.089453740 2.761028e-01 ## 159 0.121065545 1.088878345 2.763564e-01 ## 160 0.072145041 1.051155198 2.933315e-01 ## 161 0.034753997 1.048611023 2.945010e-01 ## 162 0.091912878 -1.046994297 2.952458e-01 ## 163 0.019521578 1.022300787 3.067789e-01 ## 164 0.129939291 -1.017791260 3.089169e-01 ## 165 0.149833776 -1.013891971 3.107735e-01 ## 166 0.004238263 1.007635218 3.137680e-01 ## 167 0.135313882 -1.007066299 3.140412e-01 ## 168 0.081514184 1.003618167 3.157005e-01 ## 169 0.009502469 -1.003359935 3.158250e-01 ## 170 0.048039529 0.998334306 3.182544e-01 ## 171 0.020258575 -0.973022348 3.306758e-01 ## 172 0.040870391 -0.970395051 3.319828e-01 ## 173 0.110937984 -0.966163223 3.340951e-01 ## 174 0.003408250 0.965139141 3.346076e-01 ## 175 0.015739632 0.958076392 3.381557e-01 ## 176 0.086553315 -0.957696730 3.383471e-01 ## 177 0.030421775 0.931569173 3.516868e-01 ## 178 0.012041056 0.924753761 3.552206e-01 ## 179 0.055769078 -0.913985953 3.608493e-01 ## 180 0.125221329 -0.911144249 3.623440e-01 ## 181 0.013660318 0.907928149 3.640403e-01 ## 182 0.002825724 0.894858521 3.709849e-01 ## 183 0.023389959 0.845529040 3.979303e-01 ## 184 0.044907134 0.845451211 3.979738e-01 ## 185 0.027281432 0.835874822 4.033387e-01 ## 186 0.089625539 -0.821494580 4.114757e-01 ## 187 0.004765021 0.818135673 4.133903e-01 ## 188 0.127144288 -0.810360996 4.178422e-01 ## 189 0.030071446 0.807225068 4.196458e-01 ## 190 0.018505100 -0.794324871 4.271133e-01 ## 191 0.029240246 -0.790641241 4.292598e-01 ## 192 0.012313269 -0.787574084 4.310518e-01 ## 193 0.012026663 -0.786305201 4.317944e-01 ## 194 0.051223645 0.771956916 4.402435e-01 ## 195 0.005758430 0.768555545 4.422602e-01 ## 196 0.090196280 -0.762490998 4.458691e-01 ## 197 0.076450766 0.755727445 4.499136e-01 ## 198 0.042293320 -0.701884907 4.828436e-01 ## 199 0.077628331 -0.689047980 4.908838e-01 ## 200 0.011323008 -0.684782272 4.935714e-01 ## 201 0.013137554 -0.677947428 4.978940e-01 ## 202 0.128144034 0.666126815 5.054172e-01 ## 203 0.048301742 0.661180048 5.085833e-01 ## 204 0.050615536 0.658378924 5.103807e-01 ## 205 0.014793084 0.657110247 5.111958e-01 ## 206 0.017669296 0.653720921 5.133769e-01 ## 207 0.022716110 0.644680161 5.192184e-01 ## 208 0.012952925 -0.643676532 5.198690e-01 ## 209 0.073184699 -0.639756574 5.224140e-01 ## 210 0.228771103 -0.615353348 5.384009e-01 ## 211 0.009652169 -0.603144053 5.464905e-01 ## 212 0.124633385 -0.602151183 5.471510e-01 ## 213 0.029408280 -0.593677835 5.528038e-01 ## 214 0.074269428 -0.591526648 5.542435e-01 ## 215 0.012050473 -0.591258482 5.544231e-01 ## 216 0.141371949 0.584909389 5.586835e-01 ## 217 0.048661610 -0.578157243 5.632318e-01 ## 218 0.049982145 0.560714682 5.750633e-01 ## 219 0.135502771 0.556496809 5.779419e-01 ## 220 0.101520558 0.553941870 5.796889e-01 ## 221 0.021965950 0.547719602 5.839538e-01 ## 222 0.161598039 0.542962602 5.872242e-01 ## 223 0.031493853 0.531325946 5.952599e-01 ## 224 0.107736650 -0.528122568 5.974808e-01 ## 225 0.012384183 -0.512251072 6.085395e-01 ## 226 0.054598644 -0.505945601 6.129581e-01 ## 227 0.023023130 -0.487408373 6.260296e-01 ## 228 0.026037701 -0.472037252 6.369587e-01 ## 229 0.075742351 -0.464950863 6.420241e-01 ## 230 0.100806352 -0.450351683 6.525123e-01 ## 231 0.072381428 0.450140298 6.526647e-01 ## 232 0.070120720 0.441386276 6.589875e-01 ## 233 0.002554018 0.400295580 6.889873e-01 ## 234 0.186174316 0.390121758 6.964937e-01 ## 235 0.127277480 -0.389767225 6.967558e-01 ## 236 0.009792605 -0.383300263 7.015434e-01 ## 237 0.026982885 -0.377642811 7.057414e-01 ## 238 0.040302947 0.377623854 7.057555e-01 ## 239 0.138964559 0.374125565 7.083559e-01 ## 240 0.032837873 0.373888372 7.085324e-01 ## 241 0.007512374 0.367540544 7.132600e-01 ## 242 0.033042858 -0.365871269 7.145051e-01 ## 243 0.053056220 0.359155110 7.195221e-01 ## 244 0.062809748 -0.346443569 7.290508e-01 ## 245 0.011437717 0.345407230 7.298295e-01 ## 246 0.008082355 -0.344910391 7.302029e-01 ## 247 0.063728019 -0.337826886 7.355339e-01 ## 248 0.021246406 0.335736536 7.371095e-01 ## 249 0.068621295 0.322137590 7.473867e-01 ## 250 0.055790595 0.317636733 7.507981e-01 ## 251 0.111121660 0.309443522 7.570208e-01 ## 252 0.127149784 0.300419021 7.638930e-01 ## 253 0.104061370 -0.296887432 7.665874e-01 ## 254 0.024009028 -0.296467502 7.669080e-01 ## 255 0.029652018 -0.283035662 7.771827e-01 ## 256 0.031144408 -0.279934898 7.795603e-01 ## 257 0.104212523 0.274811004 7.834937e-01 ## 258 0.070392978 -0.254808691 7.989006e-01 ## 259 0.025285075 0.250759028 8.020297e-01 ## 260 0.060764727 0.247760715 8.043484e-01 ## 261 0.047279009 0.244848737 8.066021e-01 ## 262 0.042128030 -0.243749757 8.074530e-01 ## 263 0.151416844 0.236377247 8.131674e-01 ## 264 0.012978168 -0.229148513 8.187801e-01 ## 265 0.057434889 -0.224683855 8.222513e-01 ## 266 0.133900878 0.203677236 8.386293e-01 ## 267 0.078045430 0.193705983 8.464284e-01 ## 268 0.119809577 -0.190820085 8.486885e-01 ## 269 0.009669480 0.179893356 8.572570e-01 ## 270 0.062117377 -0.166538435 8.677523e-01 ## 271 0.028300350 0.155327854 8.765806e-01 ## 272 0.076538938 -0.152109589 8.791179e-01 ## 273 0.127348943 0.148008586 8.823529e-01 ## 274 0.108812084 0.129810603 8.967311e-01 ## 275 0.120027360 0.116408681 9.073419e-01 ## 276 0.123190096 -0.101526989 9.191437e-01 ## 277 0.009071843 -0.086242162 9.312837e-01 ## 278 0.053107281 0.079442528 9.366897e-01 ## 279 0.041523455 0.061182403 9.512209e-01 ## 280 0.005375152 -0.056336419 9.550802e-01 ## 281 0.099279770 0.051734692 9.587460e-01 ## 282 0.019451967 0.048934368 9.609771e-01 ## 283 0.072346670 -0.037821669 9.698342e-01 ## 284 0.003794804 0.015608183 9.875487e-01 ## 285 0.119540961 -0.012029563 9.904034e-01 ## 286 0.013283405 -0.011579651 9.907623e-01 ## 287 0.054065051 -0.009824434 9.921625e-01 ## 288 0.084224459 0.003444895 9.972518e-01 ## 289 0.123904418 -0.001615302 9.987114e-01 ## 290 NA NA NA ## 291 NA NA NA ## 292 NA NA NA ## 293 NA NA NA ## 294 NA NA NA ## 295 NA NA NA ## 296 NA NA NA ## 297 NA NA NA ## 298 NA NA NA ## 299 NA NA NA ## 300 NA NA NA ## 301 NA NA NA ## 302 NA NA NA ## 303 NA NA NA ## 304 NA NA NA ## 305 NA NA NA ## 306 NA NA NA ## 307 NA NA NA ## 308 NA NA NA ## 309 NA NA NA ## 310 NA NA NA ## 311 NA NA NA ## 312 NA NA NA ## 313 NA NA NA ## 314 NA NA NA ## 315 NA NA NA ## 316 NA NA NA ## 317 NA NA NA ## 318 NA NA NA ## 319 NA NA NA ## 320 NA NA NA ## 321 NA NA NA ## 322 NA NA NA ## 323 NA NA NA ## 324 NA NA NA ## 325 NA NA NA ## 326 NA NA NA ## 327 NA NA NA ## 328 NA NA NA ## 329 NA NA NA ## 330 NA NA NA ## 331 NA NA NA ## 332 NA NA NA ## 333 NA NA NA ## 334 NA NA NA ## 335 NA NA NA ## 336 NA NA NA ## 337 NA NA NA ## 338 NA NA NA ## 339 NA NA NA ## 340 NA NA NA ## 341 NA NA NA ## 342 NA NA NA ## 343 NA NA NA ## 344 NA NA NA ## 345 NA NA NA ## 346 NA NA NA ## 347 NA NA NA ## 348 NA NA NA ## 349 NA NA NA ## 350 NA NA NA ## 351 NA NA NA ## 352 NA NA NA ## 353 NA NA NA ## 354 NA NA NA ## 355 NA NA NA ## 356 NA NA NA ## 357 NA NA NA ## 358 NA NA NA ## 359 NA NA NA ## 360 NA NA NA ## 361 NA NA NA ## 362 NA NA NA ## 363 NA NA NA ## 364 NA NA NA ## 365 NA NA NA ## 366 NA NA NA ## 367 NA NA NA ## 368 NA NA NA ## 369 NA NA NA ## 370 NA NA NA 6.3.6 Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paqueteria MLmetrics library(MLmetrics) MAE with(p_test,MLmetrics::MAE(.pred,Log_SalePrice)) ## [1] 15963.89 MAPE with(p_test,MLmetrics::MAPE(.pred,Log_SalePrice)) ## [1] 0.09136462 RMSE with(p_test,MLmetrics::RMSE(.pred,Log_SalePrice)) ## [1] 25234.92 \\(R^2\\) with(p_test,MLmetrics::R2_Score(.pred,Log_SalePrice)) ## [1] 0.9012817 RMSLE with(p_test,MLmetrics::RMSLE(.pred,Log_SalePrice)) ## [1] 0.1933594 6.4 Métodos se selección de variables 6.4.1 Forward selection (selección hacia adelante) Comienza sin predictores en el modelo, agrega iterativamente los predictores más contribuyentes y se detiene cuando la mejora del modelo ya no es estadísticamente significativa. La función train() del paquete caret proporciona un flujo de trabajo sencillo para realizar la selección de variables utilizando los paquetes leaps y MASS. Tiene una opción denominada método, que puede tomar los siguientes valores: LeapBackward para ajustar la regresión lineal con la selección hacia atrás leapForward para ajustar la regresión lineal con la selección hacia adelante leapSeq para ajustar la regresión lineal con la selección por pasos También es necesario especificar el parámetro de ajuste nvmax, que corresponde al número máximo de predictores que se incorporarán al modelo. Por ejemplo, nvmax puede variar de 1 a 5. En este caso, la función comienza buscando diferentes mejores modelos de diferente tamaño, hasta el mejor modelo de 5 variables. Es decir, busca el mejor modelo de 1 variable, el mejor modelo de 2 variables,  , el mejor modelo de 5 variables. Usaremos una validación cruzada de 10 veces para estimar el error de predicción promedio (RMSE) de cada uno de los 5 modelos. La métrica estadística RMSE se utiliza para comparar los 5 modelos y elegir automáticamente el mejor, donde mejor se define como el modelo que minimiza el RMSE library(caret) ## Loading required package: lattice set.seed(123) forward_model &lt;- train( Sale_Price ~ Year_Built + Bldg_Type, data = ames, method = &quot;leapForward&quot;, tuneGrid = data.frame(nvmax =1:5), trControl = trainControl(method = &quot;cv&quot;, number = 10)) forward_model$results ## nvmax RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 1 66061.59 0.3171505 46898.51 5663.063 0.04126243 2640.400 ## 2 2 64957.33 0.3398822 45981.78 5640.144 0.04117377 2570.204 ## 3 3 64422.01 0.3507725 45773.94 5516.025 0.03942256 2620.756 ## 4 4 63967.84 0.3597521 45326.43 5365.403 0.03701859 2449.530 ## 5 5 63974.95 0.3596118 45334.15 5365.339 0.03703523 2452.353 El resultado anterior muestra diferentes métricas y su desviación estándar para comparar la precisión de los 5 mejores modelos. Las columnas son: nvmax: el número de variables en el modelo. Por ejemplo nvmax = 2, especifica el mejor modelo de 2 variables. RMSE y MAE son dos métricas diferentes que miden el error de predicción de cada modelo. Cuanto menor sea el RMSE y MAE, mejor será el modelo. Rsquared (\\(R^2\\)) indica la correlación entre los valores de resultado observados y los valores predichos por el modelo. Cuanto mayor sea la \\(R^2\\), mejor será el modelo. En nuestro ejemplo, se puede ver que el modelo con 4 variables es el que tiene el RMSE más bajo. Se puede obtener el mejor valor de ajuste (nvmax), seleccionados automáticamente por la función train(), de la siguiente manera: forward_model$bestTune ## nvmax ## 4 4 La función summary() reporta el mejor conjunto de variables para cada tamaño de modelo, hasta el mejor modelo de 4 variables. summary(forward_model) ## Subset selection object ## 5 Variables (and intercept) ## Forced in Forced out ## Year_Built FALSE FALSE ## Bldg_TypeTwoFmCon FALSE FALSE ## Bldg_TypeDuplex FALSE FALSE ## Bldg_TypeTwnhs FALSE FALSE ## Bldg_TypeTwnhsE FALSE FALSE ## 1 subsets of each size up to 4 ## Selection Algorithm: forward ## Year_Built Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs ## 1 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## Bldg_TypeTwnhsE ## 1 ( 1 ) &quot; &quot; ## 2 ( 1 ) &quot; &quot; ## 3 ( 1 ) &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; Un asterisco especifica que la variable se incluye en el modelo correspondiente. 6.4.1.1 Backward selection (selección hacia atrás) Comienza con todos los predictores en el modelo (modelo completo), y elimina iterativamente los predictores menos contribuyentes y se detiene cuando tiene un modelo en el que todos los predictores son estadísticamente significativos. set.seed(123) backward_model &lt;- train( Sale_Price ~ Year_Built + Bldg_Type, data = ames, method = &quot;leapBackward&quot;, tuneGrid = data.frame(nvmax =1:5), trControl = trainControl(method = &quot;cv&quot;, number = 10)) backward_model$results ## nvmax RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 1 66061.59 0.3171505 46898.51 5663.063 0.04126243 2640.400 ## 2 2 64957.33 0.3398822 45981.78 5640.144 0.04117377 2570.204 ## 3 3 64422.01 0.3507725 45773.94 5516.025 0.03942256 2620.756 ## 4 4 63967.84 0.3597521 45326.43 5365.403 0.03701859 2449.530 ## 5 5 63974.95 0.3596118 45334.15 5365.339 0.03703523 2452.353 summary(backward_model) ## Subset selection object ## 5 Variables (and intercept) ## Forced in Forced out ## Year_Built FALSE FALSE ## Bldg_TypeTwoFmCon FALSE FALSE ## Bldg_TypeDuplex FALSE FALSE ## Bldg_TypeTwnhs FALSE FALSE ## Bldg_TypeTwnhsE FALSE FALSE ## 1 subsets of each size up to 4 ## Selection Algorithm: backward ## Year_Built Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs ## 1 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## Bldg_TypeTwnhsE ## 1 ( 1 ) &quot; &quot; ## 2 ( 1 ) &quot; &quot; ## 3 ( 1 ) &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; 6.4.1.2 Stepwise selection (selección paso a paso) Combinación de selecciones hacia adelante y hacia atrás. Comienza sin predictores, luego agrega secuencialmente los predictores más contribuyentes (como la selección hacia adelante). Después de agregar cada nueva variable, elimina cualquier variable que ya no proporcione una mejora en el ajuste del modelo (como la selección hacia atrás). set.seed(123) stepwise_model &lt;- train( Sale_Price ~ Year_Built + Bldg_Type, data = ames, method = &quot;leapSeq&quot;, tuneGrid = data.frame(nvmax =1:5), trControl = trainControl(method = &quot;cv&quot;, number = 10)) stepwise_model$results ## nvmax RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 1 66061.59 0.3171505 46898.51 5663.063 0.04126243 2640.400 ## 2 2 64957.33 0.3398822 45981.78 5640.144 0.04117377 2570.204 ## 3 3 64422.01 0.3507725 45773.94 5516.025 0.03942256 2620.756 ## 4 4 63967.84 0.3597521 45326.43 5365.403 0.03701859 2449.530 ## 5 5 63974.95 0.3596118 45334.15 5365.339 0.03703523 2452.353 summary(stepwise_model) ## Subset selection object ## 5 Variables (and intercept) ## Forced in Forced out ## Year_Built FALSE FALSE ## Bldg_TypeTwoFmCon FALSE FALSE ## Bldg_TypeDuplex FALSE FALSE ## Bldg_TypeTwnhs FALSE FALSE ## Bldg_TypeTwnhsE FALSE FALSE ## 1 subsets of each size up to 4 ## Selection Algorithm: &#39;sequential replacement&#39; ## Year_Built Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs ## 1 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## Bldg_TypeTwnhsE ## 1 ( 1 ) &quot; &quot; ## 2 ( 1 ) &quot; &quot; ## 3 ( 1 ) &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; 6.4.2 Desventajas de la selección forward, backward y stepwise Inflación de resultados falsos positivos: la selección hacia adelante, hacia atrás y paso a paso utiliza muchas pruebas de hipótesis repetidas para tomar decisiones sobre la inclusión o exclusión de predictores individuales. Los valores \\(p\\) correspondientes no están ajustados, lo que lleva a una selección excesiva de características (es decir, resultados falsos positivos). Además, este problema se agrava cuando están presentes predictores altamente correlacionados. Sobreajuste del modelo: Las estadísticas del modelo resultante, incluidas las estimaciones de los parámetros y la incertidumbre asociada, son muy optimistas ya que no tienen en cuenta el proceso de selección. 6.5 Regresión logística En esta sección aprenderemos sobre regresión logística, como se ajusta un modelo de regresión logística en R con tidymodels, las métricas de desempeño para problemas de clasificación y como podemos comparar modelos con estas métricas. Al igual que en regresión lineal, existen dos tipos de modelos de regresión logística: regresión simple y regresión múltiple. La regresión logística simple es cuando se utiliza una variable independiente para estimar la probabilidad de pertenecer a un grupo de una variable cualitativa binaria. Cuando se utiliza más de una variable independiente, el proceso se denomina regresión logística múltiple. 6.5.1 Función sigmoide Si una variable cualitativa con dos categorías se codifica como 1 y 0, matemáticamente es posible ajustar un modelo de regresión lineal por mínimos cuadrados. El problema de esta aproximación es que, al tratarse de una recta, para valores extremos del predictor, se obtienen valores de \\(Y\\) menores que 0 o mayores que 1, lo que entra en contradicción con el hecho de que las probabilidades siempre están dentro del rango [0,1]. Para evitar estos problemas, la regresión logística transforma el valor devuelto por la regresión lineal empleando una función cuyo resultado está siempre comprendido entre 0 y 1. Existen varias funciones que cumplen esta descripción, una de las más utilizadas es la función logística (también conocida como función sigmoide): \\[\\sigma(x)=\\frac{1}{1+e^{-x}}\\] Función sigmoide: Para valores de \\(x\\) muy grandes, el valor de \\(e^{-x}\\) es aproximadamente 0 por lo que el valor de la función sigmoide es 1. Para valores de \\(x\\) muy negativos, el valor \\(e^{-x}\\) tiende a infinito por lo que el valor de la función sigmoide es 0. Sustituyendo la \\(x\\) de la función sigmoide por la función lineal \\(\\beta_0+\\beta_1X\\) se obtiene que: \\[P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\\] donde \\(P(Y=k|X=x)\\) puede interpretarse como: la probabilidad de que la variable cualitativa \\(Y\\) adquiera el valor \\(k\\), dado que el predictor \\(X\\) tiene el valor \\(x\\). Esta función, puede ajustarse de forma sencilla con métodos de regresión lineal si se emplea su versión logarítmica: \\[ln(\\frac{p(Y=k|X=x)}{1p(Y=k|X=x)})=\\beta_0+\\beta_1X\\] 6.5.2 Ajuste del modelo La combinación óptima de coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) será aquella que tenga la máxima verosimilitud (maximum likelihood), es decir el valor de los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) con los que se maximiza la probabilidad de obtener los datos observados. El método de maximum likelihood está ampliamente extendido en la estadística aunque su implementación no siempre es trivial. Otra forma para ajustar un modelo de regresión logística es empleando descenso de gradiente. Si bien este no es el método de optimización más adecuado para resolver la regresión logística, está muy extendido en el ámbito del machine learning para ajustar otros modelos. Regresión logística múltiple La regresión logística múltiple es una extensión de la regresión logística simple. Se basa en los mismos principios que la regresión logística simple (explicados anteriormente) pero ampliando el número de predictores. Los predictores pueden ser tanto continuos como categóricos. \\[ln(\\frac{p}{1-p})=\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i\\] \\[logit(Y)=\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i\\] El valor de la probabilidad de Y se puede obtener con la inversa del logaritmo natural: \\[p(Y)=\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}\\] 6.5.3 Convertir probabilidad en clasificación Una de las principales aplicaciones de un modelo de regresión logística es clasificar la variable cualitativa en función de valor que tome el predictor. Para conseguir esta clasificación, es necesario establecer un threshold de probabilidad a partir de la cual se considera que la variable pertenece a uno de los niveles. Por ejemplo, se puede asignar una observación al grupo 1 si \\(p (Y=1|X)&gt;0.3\\) y al grupo 0 si ocurre lo contrario. Es importante mencionar que el punto de corte no tiene que ser 0.5, este puede ser seleccionado a convenecía de la métrica a optimizar. 6.5.4 Métricas de desempeño Existen distintas métricas de desempeño para problemas de clasificación, debido a que contamos con la respuesta correcta podemos contar cuántos aciertos tuvimos y cuántos fallos tuvimos. Primero, por simplicidad ocuparemos un ejemplo de clasificación binaria, Fraude (1) o No fraude (0). En este tipo de algoritmos definimos cuál de las categorías será nuestra etiqueta positiva y cuál será la negativa. La positiva será la categoría que queremos predecir -en nuestro ejemplo, fraude- y la negativa lo opuesto -en el caso binario- en nuestro ejemplo, no fraude. Dadas estas definiciones tenemos 4 posibilidades: True positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que es fradue. False positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que no es fraude. True negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que no es fraude. False negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que es fraude. Matriz de confusión Esta métrica corresponde a una matriz en donde se plasma el conteo de los aciertos y los errores que haya hecho el modelo. En esta métrica utilizamos todos los aciertos y todos los errores que haya tenido el modelo en las predicciones, esto es: los verdaderos positivos (TP), los verdaderos negativos (TN), los falsos positivos (FP) y los falsos negativos (FN). Normalmente los renglones representan las etiquetas reales, ya sean positivas o negativas, y las columnas, las etiquetas predichas. Accuracy Número de aciertos totales entre todas las predicciones. \\[accuracy = \\frac{TP + TN}{ TP+FP+TN+FN}\\] La métrica más utilizada, en datasets imbalanceados esta métrica no nos sirve, al contrario, nos engaña. Precision: Eficiencia De los que identificamos como clase positiva, cuántos identificamos correctamente. ¿Qué tan eficientes somos en la predicción? \\[precision = \\frac{TP}{TP + FP}\\] ¿Cuándo utilizar precision? Esta es la métrica que ocuparás más, pues en un contexto de negocio, donde los recursos son finitos y tiene un costo asociado, ya sea monetario o de tiempo o de recursos, necesitarás que las predicciones de tu etiqueta positiva sean muy eficientes. Al utilizar esta métrica estaremos optimizando el modelo para minimizar el número de falsos positivos. Recall o Sensibilidad: Cobertura Del universo posible de nuestra clase positiva, cuántos identificamos correctamente. \\[recall = \\frac{TP}{TP + FN }\\] Esta métrica la ocuparás cuando en el contexto de negocio de tu problema sea más conveniente minimizar los falsos negativos por el impacto que estos pueden tener en las personas en quienes se implementará la predicción. Al utilizar esta métrica estaremos optimizando el modelo para minimizar el número de falsos negativos. Especificidad Es el número de observaciones correctamente identificados como negativos fuera del total de negativos. \\[Specificity = \\frac{TN}{TN+FP}\\] F1-score Combina precision y recall para optimizar ambos. \\[F = 2 *\\frac{precision * recall}{precision + recall} \\] Se recomienda utilizar esta métrica de desempeño cuando quieres balancear tanto los falsos positivos como los falsos negativos. Aunque es una buena solución para tomar en cuenta ambos errores, pocas veces hay problemas reales que permiten ocuparla, esto es porque en más del 90% de los casos tenemos una restricción en recursos. Ahora con esto en mente podemos definir las siguientes métricas: AUC y ROC: Area Under the Curve y Receiver operator characteristic Una curva ROC es un gráfico que muestra el desempeño de un modelo de clasificación en todos los puntos de corte. AUC significa Área bajo la curva ROC. Es decir, AUC mide el área debajo de la curva ROC. 6.5.5 Implementación en R Ajustaremos un modelo de regresión logística usando la receta antes vista. logistic_model &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) logistic_fit1 &lt;- fit(logistic_model, Churn ~ ., telco_juiced) logistic_p_test &lt;- predict(logistic_fit1, telco_test_bake) %&gt;% bind_cols(telco_test_bake) logistic_p_test ## # A tibble: 2,113 x 29 ## .pred_class SeniorCitizen MonthlyCharges TotalCharges Churn gender_Male ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 No -0.442 -0.271 -0.175 No 1 ## 2 Yes -0.442 1.15 -0.647 Yes 0 ## 3 No -0.442 -1.18 -0.876 No 0 ## 4 No -0.442 -1.53 -0.865 No 1 ## 5 No -0.442 1.38 2.25 No 1 ## 6 Yes -0.442 0.830 -0.187 No 0 ## 7 No -0.442 -1.51 -0.920 No 1 ## 8 No -0.442 -0.183 0.302 No 1 ## 9 No -0.442 -1.16 -0.996 Yes 1 ## 10 No -0.442 0.837 1.80 No 1 ## # ... with 2,103 more rows, and 23 more variables: Partner_Yes &lt;dbl&gt;, ## # Dependents_Yes &lt;dbl&gt;, tenure_X1.2.years &lt;dbl&gt;, tenure_X2.3.years &lt;dbl&gt;, ## # tenure_X3.4.years &lt;dbl&gt;, tenure_X4.5.years &lt;dbl&gt;, tenure_X5.6.years &lt;dbl&gt;, ## # PhoneService_Yes &lt;dbl&gt;, MultipleLines_Yes &lt;dbl&gt;, ## # InternetService_Fiber.optic &lt;dbl&gt;, InternetService_No &lt;dbl&gt;, ## # OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_Yes &lt;dbl&gt;, ## # DeviceProtection_Yes &lt;dbl&gt;, TechSupport_Yes &lt;dbl&gt;, ... 6.5.6 Métricas de desempeño Matriz de Confusión yardstick::conf_mat(logistic_p_test,Churn,.pred_class) ## Truth ## Prediction No Yes ## No 1417 287 ## Yes 129 279 Accuracy (1398+296)/(1398+160+256+296) ## [1] 0.8028436 yardstick::accuracy(logistic_p_test,Churn,.pred_class, event_level = &quot;second&quot;) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.803 Precision yardstick::precision(logistic_p_test,Churn,.pred_class,event_level = &quot;second&quot;) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 precision binary 0.684 Recall yardstick::recall(logistic_p_test,Churn,.pred_class,event_level = &quot;second&quot;) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 recall binary 0.493 F1 Score with(logistic_p_test,MLmetrics::F1_Score(.pred_class,Churn),event_level = &quot;second&quot; ) ## [1] 0.872 ¿Y si se quiere un corte diferente? ¿el negocio qué necesita? logistic_p_test_prob &lt;- predict(logistic_fit1, telco_test_bake, type=&quot;prob&quot;) %&gt;% bind_cols(telco_test_bake) logistic_p_test_prob ## # A tibble: 2,113 x 30 ## .pred_No .pred_Yes SeniorCitizen MonthlyCharges TotalCharges Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.963 0.0368 -0.442 -0.271 -0.175 No ## 2 0.163 0.837 -0.442 1.15 -0.647 Yes ## 3 0.664 0.336 -0.442 -1.18 -0.876 No ## 4 0.987 0.0132 -0.442 -1.53 -0.865 No ## 5 0.965 0.0353 -0.442 1.38 2.25 No ## 6 0.491 0.509 -0.442 0.830 -0.187 No ## 7 0.924 0.0757 -0.442 -1.51 -0.920 No ## 8 0.955 0.0447 -0.442 -0.183 0.302 No ## 9 0.575 0.425 -0.442 -1.16 -0.996 Yes ## 10 0.981 0.0189 -0.442 0.837 1.80 No ## # ... with 2,103 more rows, and 24 more variables: gender_Male &lt;dbl&gt;, ## # Partner_Yes &lt;dbl&gt;, Dependents_Yes &lt;dbl&gt;, tenure_X1.2.years &lt;dbl&gt;, ## # tenure_X2.3.years &lt;dbl&gt;, tenure_X3.4.years &lt;dbl&gt;, tenure_X4.5.years &lt;dbl&gt;, ## # tenure_X5.6.years &lt;dbl&gt;, PhoneService_Yes &lt;dbl&gt;, MultipleLines_Yes &lt;dbl&gt;, ## # InternetService_Fiber.optic &lt;dbl&gt;, InternetService_No &lt;dbl&gt;, ## # OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_Yes &lt;dbl&gt;, ## # DeviceProtection_Yes &lt;dbl&gt;, TechSupport_Yes &lt;dbl&gt;, ... logistic_p_test_prob &lt;- logistic_p_test_prob %&gt;% mutate(.pred_class = as_factor(if_else ( .pred_Yes &gt;= 0.30, &#39;Yes&#39;, &#39;No&#39;))) %&gt;% relocate(.pred_class , .before = .pred_No) logistic_p_test_prob ## # A tibble: 2,113 x 31 ## .pred_class .pred_No .pred_Yes SeniorCitizen MonthlyCharges TotalCharges ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 0.963 0.0368 -0.442 -0.271 -0.175 ## 2 Yes 0.163 0.837 -0.442 1.15 -0.647 ## 3 Yes 0.664 0.336 -0.442 -1.18 -0.876 ## 4 No 0.987 0.0132 -0.442 -1.53 -0.865 ## 5 No 0.965 0.0353 -0.442 1.38 2.25 ## 6 Yes 0.491 0.509 -0.442 0.830 -0.187 ## 7 No 0.924 0.0757 -0.442 -1.51 -0.920 ## 8 No 0.955 0.0447 -0.442 -0.183 0.302 ## 9 Yes 0.575 0.425 -0.442 -1.16 -0.996 ## 10 No 0.981 0.0189 -0.442 0.837 1.80 ## # ... with 2,103 more rows, and 25 more variables: Churn &lt;fct&gt;, ## # gender_Male &lt;dbl&gt;, Partner_Yes &lt;dbl&gt;, Dependents_Yes &lt;dbl&gt;, ## # tenure_X1.2.years &lt;dbl&gt;, tenure_X2.3.years &lt;dbl&gt;, tenure_X3.4.years &lt;dbl&gt;, ## # tenure_X4.5.years &lt;dbl&gt;, tenure_X5.6.years &lt;dbl&gt;, PhoneService_Yes &lt;dbl&gt;, ## # MultipleLines_Yes &lt;dbl&gt;, InternetService_Fiber.optic &lt;dbl&gt;, ## # InternetService_No &lt;dbl&gt;, OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_Yes &lt;dbl&gt;, ## # DeviceProtection_Yes &lt;dbl&gt;, TechSupport_Yes &lt;dbl&gt;, ... cm_5&lt;- yardstick::conf_mat(logistic_p_test,Churn,.pred_class) cm_3&lt;- yardstick::conf_mat(logistic_p_test_prob,Churn,.pred_class) cm_5 ## Truth ## Prediction No Yes ## No 1417 287 ## Yes 129 279 cm_3 ## Truth ## Prediction No Yes ## No 1181 124 ## Yes 365 442 Precision yardstick::precision(logistic_p_test_prob,Churn,.pred_class,event_level = &quot;second&quot;) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 precision binary 0.548 Recall yardstick::recall(logistic_p_test_prob,Churn,.pred_class, event_level = &quot;second&quot;) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 recall binary 0.781 Para poder determinar cual es el mejor punto de corte, es indispensable conocer el comportamiento y efecto de los diferentes puntos de corte. Veamos un ejemplo visual en nuestra aplicación de Shiny: ConfusionMatrixShiny 6.6 Regresión regularizada 6.7 KNN 6.8 Tree decision 6.9 Bagging 6.10 Random forest 6.11 Boosting "],["machine-learning-aprendizaje-no-supervisado.html", "Capítulo 7 MACHINE LEARNING: APRENDIZAJE NO SUPERVISADO 7.1 Cluster: K- means 7.2 Cluster: PAM 7.3 DBSCAN 7.4 Cluster: Jerárquico", " Capítulo 7 MACHINE LEARNING: APRENDIZAJE NO SUPERVISADO 7.1 Cluster: K- means 7.2 Cluster: PAM 7.3 DBSCAN 7.4 Cluster: Jerárquico "]]
